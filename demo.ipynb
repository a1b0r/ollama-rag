{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "RAG (Retrieval-Augmented Generation) combines information retrieval and language generation in machine learning. It retrieves relevant data before generating responses, enhancing the model's output quality and knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RAG (Retrieval-Augmented Generation) is a type of language model architecture that combines the capabilities of both generation and retrieval models. The goal of RAG is to improve the quality and relevance of generated text by incorporating information from a reference corpus, such as a knowledge base or a database of relevant texts.\n",
    "\n",
    "In a traditional language model, the generator network takes a random noise vector as input and generates text based on a learned probability distribution. In contrast, RAG adds an additional retrieval network that is trained to retrieve relevant passages from the reference corpus and use them to augment the generator's output. This allows the generator to learn from the reference corpus and generate more informative and accurate text.\n",
    "\n",
    "RAG consists of two main components: a question encoder and a retriever. The question encoder takes in a question or prompt and outputs a vector representation that captures the meaning of the question. The retriever then uses this vector to retrieve relevant passages from the reference corpus, which are then used to augment the generator's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 0. Imports and model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "install [requirements](requirements.txt) and [README](README.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr  # for creating web UIs for Python applications\n",
    "from langchain_community.document_loaders import WebBaseLoader  # document loaders for web and PDF sources\n",
    "from langchain_community.vectorstores import Chroma  #v ector store for document embeddings\n",
    "from langchain_community import embeddings  # for generating embeddings\n",
    "from langchain_community.chat_models import ChatOllama  # model for generating responses\n",
    "from langchain_core.runnables import RunnablePassthrough  # for passing arguments as-is in pipelines\n",
    "from langchain_core.output_parsers import StrOutputParser  # for parsing outputs to strings\n",
    "from langchain_core.prompts import ChatPromptTemplate  # for creating chat prompts\n",
    "from langchain.output_parsers import PydanticOutputParser  # for parsing outputs using Pydantic models\n",
    "from langchain.text_splitter import CharacterTextSplitter  # for splitting text based on character count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://python.langchain.com/docs/get_started/introduction) is a framework for developing applications powered by language models. It enables applications that:\n",
    "* Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "* Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This framework consists of several parts.  "
   ]
  },
  {
   "attachments": {
    "langchain_stack.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![langchain stack](attachment:langchain_stack.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama2\", temperature=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "*,\n",
    "    name: str | None = None,\n",
    "    base_url: str = \"http://localhost:11434\",\n",
    "    model: str = \"llama2\",\n",
    "    mirostat: int | None = None,\n",
    "    mirostat_eta: float | None = None,\n",
    "    mirostat_tau: float | None = None,\n",
    "    num_ctx: int | None = None,\n",
    "    num_gpu: int | None = None,\n",
    "    num_thread: int | None = None,\n",
    "    repeat_last_n: int | None = None,\n",
    "    repeat_penalty: float | None = None,\n",
    "    temperature: float | None = None,\n",
    "    stop: List[str] | None = None,\n",
    "    tfs_z: float | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: int | None = None,\n",
    "    system: str | None = None,\n",
    "    template: str | None = None,\n",
    "    format: str | None = None,\n",
    "    timeout: int | None = None,\n",
    "    cache: bool | None = None,\n",
    "    verbose: bool = _get_verbosity,\n",
    "    callbacks: Callbacks = None,\n",
    "    callback_manager: BaseCallbackManager | None = None,\n",
    "    tags: List[str] | None = None,\n",
    "    metadata: Dict[str, Any] | None = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://github.com/gkamradt/LLMTest_NeedleInAHaystack\",\n",
    "    \"https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38\",\n",
    "    \"https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html\",\n",
    "    'https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        gkamradt\\n \\n/\\n\\nLLMTest_NeedleInAHaystack\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n\\n\\n\\n \\n\\nFork\\n    61\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 656\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nLicense\\n\\n\\n\\n\\n\\n     View license\\n    \\n\\n\\n\\n\\n\\n\\n656\\n          stars\\n \\n\\n\\n\\n61\\n          forks\\n \\n\\n\\n\\nBranches\\n \\n\\n\\n\\nTags\\n \\n\\n\\n\\nActivity\\n \\n\\n\\n\\n \\n\\n\\n          Star\\n\\n  \\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n1\\n\\n\\n\\n\\n\\n\\nPull requests\\n3\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n0\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Projects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\ngkamradt/LLMTest_NeedleInAHaystack\\n\\n\\n\\n\\n\\n\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\xa0mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History15 CommitsPaulGrahamEssaysPaulGrahamEssays\\xa0\\xa0imgimg\\xa0\\xa0original_resultsoriginal_results\\xa0\\xa0vizviz\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0Anthropic_prompt.txtAnthropic_prompt.txt\\xa0\\xa0LICENSE.txtLICENSE.txt\\xa0\\xa0LLMNeedleHaystackTester.pyLLMNeedleHaystackTester.py\\xa0\\xa0README.mdREADME.md\\xa0\\xa0requirements.txtrequirements.txt\\xa0\\xa0View all filesRepository files navigationREADMELicenseNeedle In A Haystack - Pressure Testing LLMs\\nSupported model providers: OpenAI, Anthropic\\nA simple 'needle in a haystack' analysis to test in-context retrieval ability of long context LLMs.\\nGet the behind the scenes on the overview video.\\n\\ngit clone https://github.com/gkamradt/LLMTest_NeedleInAHaystack.git\\n\\nThe Test\\n\\nPlace a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack')\\nAsk the model to retrieve this statement\\nIterate over various document depths (where the needle is placed) and context lengths to measure performance\\n\\nThis is the code that backed this OpenAI and Anthropic analysis.\\nIf ran and save_results = True, then this script will populate a result/ directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.\\nI've put the results from the original tests in /original_results. I've upgraded the script since those test were ran so the data formats may not match your script results.\\nThe key parameters:\\n\\nneedle - The statement or fact which will be placed in your context ('haystack')\\nhaystack_dir - The directory which contains the text files to load as background context. Only text files are supported\\nretrieval_question - The question with which to retrieve your needle in the background context\\nresults_version - You may want to run your test multiple times for the same combination of length/depth, change the version number if so\\ncontext_lengths_min - The starting point of your context lengths list to iterate\\ncontext_lengths_max - The ending point of your context lengths list to iterate\\ncontext_lengths_num_intervals - The number of intervals between your min/max to iterate through\\ndocument_depth_percent_min - The starting point of your document depths. Should be int > 0\\ndocument_depth_percent_max - The ending point of your document depths. Should be int < 100\\ndocument_depth_percent_intervals - The number of iterations to do between your min/max points\\ndocument_depth_percent_interval_type - Determines the distribution of depths to iterate over. 'linear' or 'sigmoid\\nmodel_provider - 'OpenAI' or 'Anthropic'\\nmodel_name - The name of the model you'd like to test. Should match the exact value which needs to be passed to the api. Ex: gpt-4-1106-preview\\nsave_results - Whether or not you'd like to save your results to file. They will be temporarily saved in the object regardless. True/False\\nsave_contexts - Whether or not you'd like to save your contexts to file. Warning these will get very long. True/False\\n\\nOther Parameters:\\n\\ncontext_lengths - A custom set of context lengths. This will override the values set for context_lengths_min, max, and intervals if set\\ndocument_depth_percents - A custom set of document depths lengths. This will override the values set for document_depth_percent_min, max, and intervals if set\\nopenai_api_key - Must be supplied. GPT-4 is used for evaluation. Can either be passed when creating the object or an environment variable\\nanthropic_api_key - Only needed if testing Anthropic models. Can either be passed when creating the object or an environment variable\\nnum_concurrent_requests - Default: 1. Set higher if you'd like to run more requests in parallel. Keep in mind rate limits.\\nfinal_context_length_buffer - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.\\nseconds_to_sleep_between_completions - Default: None, set # of seconds if you'd like to slow down your requests\\nprint_ongoing_status - Default: True, whether or not to print the status of test as they complete\\n\\nResults Visualization\\nLLMNeedleInHaystackVisualization.ipynb holds the code to make the pivot table visualization. The pivot table was then transferred to Google Slides for custom annotations and formatting. See the google slides version. See an overview of how this viz was created here.\\nOpenAI's GPT-4-128K (Run 11/8/2023)\\n\\nAnthropic's Claude 2.1 (Run 11/21/2023)\\n\\nLicense\\nThis project is licensed under the MIT License - see the LICENSE file for details. Use of this software requires attribution to the original author and project, as detailed in the license.\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nResources\\n\\n\\n\\n\\n\\n        Readme\\n \\nLicense\\n\\n\\n\\n\\n\\n     View license\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\nActivity\\n \\nStars\\n\\n\\n\\n\\n\\n656\\n      stars\\n \\nWatchers\\n\\n\\n\\n\\n\\n9\\n      watching\\n \\nForks\\n\\n\\n\\n\\n\\n61\\n      forks\\n \\n\\n\\n          Report repository\\n \\n\\n\\n\\n\\n\\n\\n\\n    Releases\\n\\nNo releases published\\n\\n\\n\\n\\n\\n\\n    Packages\\n      0\\n\\n\\n        No packages published \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Contributors\\n      2\\n\\n\\n\\n\\n\\n\\n\\n\\ngkamradt\\n\\n \\n\\n\\n\\n\\n\\n\\neltociear\\nIkko Eltociear Ashimine\\n\\n \\n\\n\\n\\n\\n\\nLanguages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJupyter Notebook\\n87.1%\\n\\n\\n\\n\\n\\n\\n\\nPython\\n12.9%\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://github.com/gkamradt/LLMTest_NeedleInAHaystack', 'title': 'GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy', 'description': 'Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack', 'language': 'en'})],\n",
       " [Document(page_content='The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inImage created by author using Dall-E 3The Needle In a Haystack TestEvaluating the performance of RAG systemsAparna Dhinakaran·FollowPublished inTowards Data Science·9 min read·Feb 15, 2024--ListenShareMy thanks to Greg Kamradt and Evan Jolley for their contributions to this pieceRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses.RAG evaluation, therefore, has become a critical part in the development and deployment of these systems. One new innovative approach to this challenge is the “Needle in a Haystack’’ test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here. This test is designed to evaluate the performance of RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.Often in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?We ran this test multiple times across several major language models. Let’s take a closer look at the process and overall results.TakeawaysNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.Minute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.When building on top of LLMs — especially when those models are connected to private data — it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance.Understanding the Needle In a Haystack TestThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:Figure 1: About 120 tokens and 50% depth | Image by Greg Kamradt on X, used here with author’s permissionThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:Figure 2: ChatGPT-4’s performance | Image by Greg Kamradt on X, used here with author’s permissionAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the “needle” is positioned towards the beginning of the context, the model tends to overlook or “forget” it — whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid.Figure 3: Claude 2.1’s performance | | Image by Greg Kamradt on X, used here with author’s permissionFor Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.Anthropic’s ResponseIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.First, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, along with an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.Second, a small edit was made to the prompt template used to query the model. A single line — here is the most relevant sentence in the context — was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.PROMPT = \"\"\"HUMAN: <context>{context}</context>What is the most fun thing to do in San Francisco based on the context? Don\\'t give information outside the document or repeat our findingsAssistant: here is the most relevant sentence in the context:\"\"\"These changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Finding this initial research fascinating, we decided to run our own set of experiments using the Needle in a Haystack test.Further ExperimentsIn conducting a new series of tests, we implemented several modifications to the original experiments. The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our open source Phoenix evals library (full disclosure: I lead the team that built Phoenix) to reduce the testing time and use rails to search directly for the random number in the output, cutting through wordiness that would decrease a retrieval score. Finally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.The updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral AI’s Mixtral-8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, we used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:SIMPLE_TEMPLATE = \\'\\'\\'    You are a helpful AI bot that answers questions for a user. Keep your responses short and direct.    The following is a set of context and a question that will relate to the context.   #CONTEXT   {context}   #ENDCONTEXT   #QUESTION   {question} Don’t give information outside the document or repeat your findings. If the information is not available in the context respond UNANSWERABLEFor Claude, we tested both previously discussed templates.ANTHROPIC_TEMPLATE_ORIGINAL = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I’m going to give you the text of some essays. Amidst the essays (“the haystack”) I’ve inserted a sentence (“the needle”) that contains an answer to the user’s question. Here\\'s the question:   <question>{question}</question>   Here’s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you’ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can’t find the answer return the single word UNANSWERABLE   Assistant: \\'\\'\\'ANTHROPIC_TEMPLATE_REV2 = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I\\'m going to give you the text of some essays. Amidst the essays (\"the haystack\") I\\'ve inserted a sentence (\"the needle\") that contains an answer to the user\\'s question. Here\\'s the question:   <question>{question}</question>   Here\\'s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you\\'ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can\\'t find the answer return the single word UNANSWERABLE   Assistant: Here is the most relevant sentence in the context:\\'\\'\\'All code run to complete these tests can be found in this GitHub repository.ResultsFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2) | Image by authorFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2 | Image by authorOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.Figure 9: Comparison of Claude 2.1 results with and without prompting guidanceAlthough we were not able to replicate Anthropic’s results of 98% retrieval accuracy for Claude 2.1 with prompting guidance, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.Figure 10: Mixtral results | Image by authorLast but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evaluations.ConclusionThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral MOE greatly outperformed our expectations, and we are excited to see Mixtral models continually overperform expectations.Llm EvaluationRetrieval AugmentedMistralOpenAIHands On Tutorials----FollowWritten by Aparna Dhinakaran2.3K Followers·Writer for Towards Data ScienceCo-Founder and CPO of Arize AI. Formerly Computer Vision PhD at Cornell, Uber Machine Learning, UC Berkeley AI Research.FollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38', 'title': 'The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data Science', 'description': 'Retrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. RAG…', 'language': 'en'})],\n",
       " [Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNeedle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\nHomepage\\n\\n\\n\\n\\n                Leaderboard\\n              \\n\\n\\nLLM \\n\\n\\n\\nVLM \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\n\\n\\n\\n            latest\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\nGet Started\\n\\nInstallation\\nDataset Preparation\\nQuick Start\\nFAQ\\n\\nUser Guides\\n\\nOverview\\nLearn About Config\\nConfigure Datasets\\nPrepare Models\\nEfficient Evaluation\\nTask Execution and Monitoring\\nMetric Calculation\\nResults Summary\\n\\nPrompt\\n\\nPrompt Overview\\nPrompt Template\\nMeta Template\\nChain of Thought\\n\\nAdvanced Guides\\n\\nAdd a dataset\\nCustom Dataset Tutorial\\nAdd a Model\\nEvaluation with LMDeploy\\nEvaluation with Lightllm\\nCode Evaluation Tutorial\\nCode Evaluation Docker Tutorial\\nMulti-modality Evaluation\\nPrompt Attack\\nLong Context Evaluation Guidance\\nSubjective Evaluation Guidance\\nCircularEval\\nData Contamination Assessment\\nNeedle In A Haystack Experiment Evaluation\\n\\nTools\\n\\nUseful Tools\\n\\nNotes\\n\\nContributing to OpenCompass\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Docs\\n         >\\n      \\nNeedle In A Haystack Experiment Evaluation\\n\\n 以中文阅读\\n\\n\\n\\n\\n\\n\\n\\n        Shortcuts\\n      \\n\\n\\n\\n\\n\\n\\n\\nNeedle In A Haystack Experiment Evaluation¶\\n\\nIntroduction to the Needle In A Haystack Test¶\\nThe Needle In A Haystack test, inspired by NeedleInAHaystack, is a method to evaluate the long-text information extraction ability of Large Language Models (LLMs). It involves randomly inserting key information at various points in a long text to form a prompt for LLMs. This test assesses the fundamental ability of LLMs to understand long texts by extracting critical information from them.\\n\\n\\nDataset Introduction¶\\nThe Skywork/ChineseDomainModelingEval dataset includes high-quality Chinese articles published between September and October 2023, covering multiple domains. These articles ensure a fair and challenging benchmark for testing.\\n\\n\\nFile Description¶\\nThe dataset includes files specific to various domains:\\n\\nzh_finance.jsonl - Finance\\nzh_game.jsonl - Gaming\\nzh_government.jsonl - Government\\nzh_movie.jsonl - Movies\\nzh_tech.jsonl - Technology\\nzh_general.jsonl - General\\n\\nThese files are used to assess the LLM’s understanding of different specific domains.\\n\\nEvaluation Steps¶\\n\\nDownload the dataset from Skywork/ChineseDomainModelingEval.\\nPlace the downloaded files in opencompass/data/CDME/. The expected file structure in the CDME directory is as follows:\\nopencompass/\\n├── configs\\n├── docs\\n├── data\\n│   └── CDME\\n│       ├── processed\\n│       ├── README.md\\n│       ├── zh_finance.jsonl\\n│       ├── zh_game.jsonl\\n│       ├── zh_general.jsonl\\n│       ├── zh_government.jsonl\\n│       ├── zh_movie.jsonl\\n│       └── zh_tech.jsonl\\n├── LICENSE\\n├── opencompass\\n├── outputs\\n├── run.py\\n├── more...\\n\\n\\n\\n\\n\\n\\nEnvironment Setup¶\\nconda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y\\nconda activate opencompass\\ngit clone https://github.com/open-compass/opencompass opencompass\\ncd opencompass\\npip install -e .\\n\\n\\n\\n\\nConfiguring the Dataset¶\\nIn the latest version, datasets are no longer generated by running scripts but dynamically defined and loaded through configuration files. Users need to specify dataset parameters in the configuration file according to their needs, offering greater flexibility and customization options.\\n\\nDataset Configuration Example¶\\nHere is an example of dataset configuration, showing how to define a dataset in the configs/datasets/cdme/cdme8k.py configuration file. This example demonstrates a Chinese dataset configuration with a length of 8000 tokens:\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            \\'abbr\\': f\\'CDME_Length{original_context_length}Depth{int(depth_percent)}\\',\\n            \\'type\\': CDMEDataset,\\n            \\'path\\': base_path,\\n            \\'length\\': original_context_length,\\n            \\'depth\\': int(depth_percent),\\n            \\'tokenizer_model\\': \\'gpt-4\\',\\n            \\'file_list\\': file_list,\\n            \\'num_repeats_per_file\\': 10,\\n            \\'length_buffer\\': 200,\\n            \\'guide\\': True,\\n            \\'language\\': \\'Chinese\\',\\n            \\'needle\\': \\'\\\\n小明最喜欢的实习的地点就是上海人工智能实验室。\\\\n\\',\\n            \\'retrieval_question\\': \\'小明最喜欢的实习地点是哪里？请按照“小明最喜欢的实习地点就是________。”的格式回答。\\',\\n            \\'reader_cfg\\': cdme_reader_cfg,\\n            \\'infer_cfg\\': cdme_infer_cfg,\\n            \\'eval_cfg\\': cdme_eval_cfg\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, the main parameters include:\\n\\nabbr: Abbreviation of the dataset.\\ntype: Dataset type.\\npath: Path to the dataset files.\\nlength: Context length in tokens.\\ndepth: Depth percentage of the document.\\ntokenizer_model: Tokenizer model used.\\nfile_list: List of data source files.\\nnum_repeats_per_file: Number of repeats per file.\\nlength_buffer: Length buffer.\\nguide: Whether it’s a guided dataset.\\nlanguage: Language of the dataset.\\nneedle: Specific text to find in the dataset (the ‘needle’).\\nretrieval_question: Question used to prompt the model for retrieval.\\nreader_cfg, infer_cfg, eval_cfg: Configurations for reading, inference, and evaluation, respectively.\\n\\nBy defining these parameters in the configuration file, you can flexibly create datasets that suit your needs. Configuration files offer a highly customizable and scalable way to manage the generation and use of datasets.\\n\\n\\n\\nMulti-Needle Needle In A Haystack Test¶\\nThe latest version introduces the multi-needle Needle In A Haystack test, allowing multiple different needles (text snippets) to be inserted into the same dataset. These needles are inserted in sequence according to a given depth parameter. Compared to the single-needle test, the multi-needle test provides a more complex data processing scenario.\\n\\nMulti-Needle Dataset Configuration Example¶\\nHere is an example of configuring a multi-needle dataset, showing how to define a multi-needle dataset in the configs/datasets/cdme/multi_needle/cdme8k_cot3_italy.py configuration file. This example demonstrates a dataset configuration with three needles:\\n# Basic dataset configuration\\nbase_path = \\'./data/CDME\\'\\nfile_list = [\\'zh_finance.jsonl\\']\\n\\n# Definition of Needles\\nneedles = [\\n    \\'\\\\n意大利的佛罗伦萨有一家名为\"La Giostra\"的餐馆，是整个佛罗伦萨中排行第一的餐馆。\\\\n\\',\\n    \\'\"La Giostra\"餐馆的特色菜肴是松露奶酪通心粉。\\',\\n    \\'松露奶酪通心粉是该家餐馆的有着意大利皇室烹饪血统的大厨Jack制作\\'\\n]\\n\\n\\n# Configuration parameters\\nretrieval_question = (\"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫什么？\"\\n                      \"请按照\\'制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫______。\\'的格式回答。\")\\nanswer = \"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫Jack\"\\nkeyword = \"Jack\"\\ndiff = 25\\n\\n# Dataset generation loop\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            # Other configuration items...\\n            \\'needles\\': needles,\\n            \\'diff\\': diff,\\n            \\'keyword\\': keyword,\\n            # Other configuration items...\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, in addition to the standard parameters, the main new parameters include:\\n\\nneedles: A list containing multiple strings, each representing a needle to be inserted.\\ndiff: Defines the depth increment for subsequent needles relative to the first needle.\\nkeyword: A keyword used for score correction during the evaluation process.\\n\\n\\n\\nChange in Scoring Mechanism¶\\nIn the source code of opencompass/datasets/cdme/cdme_multi.py, the scoring mechanism for multi-needle datasets differs. The following code segment has been added to adjust the scores based on the keyword in the predictions:\\nif keyword in prediction:\\n    print(f\\'{keyword} is in {prediction}\\')\\n    score = 100\\nelse:\\n    print(f\\'{keyword} is not in {prediction}\\')\\n    score = 0.2 * score\\n\\n\\nThis code means that if the keyword is present in the prediction, it will be awarded a high score (e.g., 100). If not, the score will be significantly reduced (20% of the original score). This scoring mechanism places more emphasis on the accuracy of keywords, supplementing the traditional scoring methods.\\n\\n\\n\\nEvaluation¶\\n\\nEvaluating with the internlm Model¶\\nFor example, to evaluate using the internlm model, the following command can be used:\\npython run.py configs/eval_needleinahaystack.py --slurm -p partition_name -q auto --max-num-workers 32\\n\\n\\nThis command initiates the evaluation process, where the model attempts to find the specified “needle” in the generated dataset. The parameters -p partition_name -q auto and --max-num-workers 32 specify the Slurm queue and the maximum number of worker processes, respectively.\\n\\n\\nLarge-Scale Text Evaluation with LMDeploy¶\\nWhen evaluating especially long texts (e.g., 200k tokens), conventional methods might lead to memory overload. In such cases, quantized models can be used for evaluation. This can be achieved using the LMDeploy tool (LMDeploy).\\nDetailed information about installing and configuring LMDeploy can be found on its GitHub page. Once installed, the TurboMindModel defined in the configs/eval_needleinahaystack_turbomind.py configuration file can be used for evaluation.\\nBelow is an example configuration in the configs/eval_needleinahaystack_turbomind.py file:\\nfrom opencompass.models.turbomind import TurboMindModel\\nfrom mmengine.config import read_base\\n\\nwith read_base():\\n    from .datasets.cdme.cdme200k import cdme_datasets\\n\\ndatasets = [*cdme_datasets]\\n\\ninternlm_meta_template = dict(round=[\\n    dict(role=\\'HUMAN\\', begin=\\':\\', end=\\'\\\\n\\'),\\n    dict(role=\\'BOT\\', begin=\\':\\', end=\\'<eoa>\\\\n\\', generate=True),\\n],\\n                              eos_token_id=103028)\\n\\nmodels = [\\n    dict(\\n        type=TurboMindModel,\\n        abbr=\\'internlm-chat-20b-turbomind\\',\\n        path=\\'./turbomind\\',\\n        max_out_len=100,\\n        max_seq_len=2048,\\n        batch_size=8,\\n        concurrency=8,\\n        meta_template=internlm_meta_template,\\n        run_cfg=dict(num_gpus=1, num_procs=1),\\n    )\\n]\\n\\n\\nIn this configuration, the TurboMindModel combines the functionality of LMDeploy, suitable for handling large-scale text datasets and effectively reducing memory usage.\\n\\n\\n\\nScore Calculation Method¶\\nIn the CDMEEvaluator class, we use two main methods to calculate scores: levenshtein_distance and score. Here are detailed explanations and implementations of these methods.\\n\\nLevenshtein Distance¶\\nLevenshtein distance is a measure of the difference between two strings. It represents the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\\ndef levenshtein_distance(self, s1, s2):\\n    if len(s1) < len(s2):\\n        return self.levenshtein_distance(s2, s1)\\n\\n    if len(s2) == 0:\\n        return len(s1)\\n\\n    previous_row = range(len(s2) + 1)\\n    for i, c1 in enumerate(s1):\\n        current_row = [i + 1]\\n        for j, c2 in enumerate(s2):\\n            insertions = previous_row[j + 1] + 1\\n            deletions = current_row[j] + 1\\n            substitutions = previous_row[j] + (c1 != c2)\\n            current_row.append(min(insertions, deletions, substitutions))\\n        previous_row = current_row\\n\\n    return previous_row[-1]\\n\\n\\n\\n\\nScore Calculation¶\\nThe score calculation method accepts two lists of predictions and references and calculates the edit distance and score for each pair of prediction and reference.\\ndef score(self, predictions, references):\\n    if len(predictions) != len(references):\\n        return {\"error\": \"predictions and references have different lengths\"}\\n\\n    total_score = 0\\n    details = []\\n    for prediction, reference in zip(predictions, references):\\n        prediction = re.sub(r\\'\\\\s+\\', \\'\\', prediction)\\n        reference = re.sub(r\\'\\\\s+\\', \\'\\', reference)\\n        edit_distance = self.levenshtein_distance(prediction, reference)\\n\\n\\n        max_len = max(len(prediction), len(reference))\\n        score = 100 * (1 - edit_distance /max_len) if max_len != 0 else 100\\n\\n        detail = {\\n            \"pred\": prediction,\\n            \"ref\": reference,\\n            \"edit_distance\": edit_distance,\\n            \"score\": score\\n        }\\n        total_score += score\\n        details.append(detail)\\n\\n    average_score = total_score / len(predictions) if predictions else 0\\n    result = {\"average_score\": average_score, \"details\": details}\\n    return result\\n\\n\\nThis scoring method first removes all whitespace characters from both predictions and references and then calculates the Levenshtein distance between them. The score is calculated as 100 minus the percentage loss based on edit distance. Finally, it returns detailed scores for each prediction and the average score overall.\\n\\n\\n\\nVisualization¶\\nThe tools_needleinahaystack.py script can be used to visualize CSV files. This script supports specifying one or more CSV file paths through the --path parameter and can use the --dataset_length parameter to specify the length of the dataset.\\n\\nUsage Examples¶\\nTo visualize a single CSV file:\\npython tools/tools_needleinahaystack.py --path \\'outputs/default/20231216_161457/summary/summary_20231216_161457.csv\\'\\n\\n\\nTo visualize multiple CSV files:\\npython tools/tools_needleinahaystack.py --path \\'path_to_first_csv.csv\\' \\'path_to_second_csv.csv\\'\\n\\n\\nTo specify the dataset length for visualization, which is used for generating titles in the visualization charts:\\npython tools/tools_needleinahaystack.py --path \\'path_to_csv.csv\\' --dataset_length 200K\\n\\n\\nCurrently, this approach only supports the CDME dataset, and we welcome community contributions for more datasets.\\nIf you use this method, please cite as follows:\\n@misc{2023opencompass,\\n    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},\\n    author={OpenCompass Contributors},\\n    howpublished={\\\\url{https://github.com/open-compass/opencompass}},\\n    year={2023}\\n}\\n\\n@misc{LLMTest_NeedleInAHaystack,\\n  title={LLMTest Needle In A Haystack - Pressure Testing LLMs},\\n  author={gkamradt},\\n  year={2023},\\n  howpublished={\\\\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}}\\n}\\n\\n@misc{wei2023skywork,\\n      title={Skywork: A More Open Bilingual Foundation Model},\\n      author={Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei Lü and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and Xuejie Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},\\n      year={2023},\\n      eprint={2310.19341},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNext \\n Previous\\n\\n\\n\\n\\n      © Copyright 2023, OpenCompass.\\n      \\n      \\n        Revision 120bf8b3.\\n      \\n\\n\\n\\n    Built with Sphinx using a theme provided by Read the\\n      Docs.\\n  \\n\\n\\n\\n\\n\\n\\n\\nNeedle In A Haystack Experiment Evaluation\\nIntroduction to the Needle In A Haystack Test\\nDataset Introduction\\nFile Description\\nEvaluation Steps\\nEnvironment Setup\\nConfiguring the Dataset\\nDataset Configuration Example\\n\\n\\nMulti-Needle Needle In A Haystack Test\\nMulti-Needle Dataset Configuration Example\\nChange in Scoring Mechanism\\n\\n\\nEvaluation\\nEvaluating with the internlm Model\\nLarge-Scale Text Evaluation with LMDeploy\\n\\n\\nScore Calculation Method\\nLevenshtein Distance\\nScore Calculation\\n\\n\\nVisualization\\nUsage Examples\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Read the Docs\\n      v: latest\\n      \\n\\n\\n\\nVersions\\nlatest\\nstable\\n\\n\\nDownloads\\nepub\\n\\n\\nOn Read the Docs\\n\\nProject Home\\n\\n\\nBuilds\\n\\n\\n\\n      Free document hosting provided by Read the Docs.\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n@沪ICP备2021009351号-23\\nOpenCompass Open Platform Service Agreement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html', 'title': 'Needle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation', 'language': 'en'})],\n",
       " [Document(page_content=\"\\n\\n\\n\\n\\n\\n\\nThe Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFree LLM Observability Courses |\\xa0Enroll now!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlatform\\n\\nOverview\\nLeft ColumnMonitorsDashboardsEval & Performance TracingExplainability & Fairness\\nRight ColumnEmbeddings & RAG AnalyzerLLM TracingFine TunePhoenix OSS\\nArize Product Demo: See the Platform in ActionWatch now\\n\\n\\nSolutions\\n\\nOverview\\nLeft ColumnUse CasesComputer VisionRecommender SystemsRegression & ClassificationForecasting\\nRight ColumnIndustriesFinancial ServiceseCommerceMedia & EntertainmentAutonomous VehiclesManufacturingBiotechnology & Pharmaceutical Research\\nCustomersLearn more\\n\\n\\nPricing\\n\\n\\nLearn\\n\\nLeft ColumnResourcesBlogPodcastEventsVideosPaper Readings\\nRight ColumnArize UniversityCertificationBasic and advanced course certificationLLMOpsSelf-guided LLMops learningML ObservabilitySelf-guided observability learning\\nArize CommunityGreat discussions, support, and random acts of swag await!\\nJoin now\\n\\n\\nDocs\\n\\n\\nCompany\\n\\nAbout\\nPartners\\nCareers\\nPress\\nSecurity\\nArize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nSign Up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n×\\nARIZE GENERATIVE AI COURSE\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tIntro To LLMOps\\t\\t\\n\\nIntro To LLMOps and LLM Observability\\n\\n\\nWhat is LLM Observability?\\n\\nIntroduction To LLMOps\\n\\nFoundation Models\\n\\nOpenAI API\\n\\nLLM Deployment\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Evaluation: Build and Benchmark Evals\\n\\nThe Needle In a Haystack Test\\n\\nEvals from OpenAI\\n\\n\\nTraces and Spans\\n\\n\\nTraces and Spans in Orchestration Frameworks\\n\\n\\nSearch and Retrieval: Build Your Own Chatbot\\n\\n\\nRetrieval Augmented Generation (RAG)\\n\\nBenchmarking Evaluation of LLM RAG\\n\\nRAG Evaluation\\n\\n\\nPrompt Engineering\\n\\n\\nEvaluating Prompts: A Developer’s Guide\\n\\n\\nChains and Agents\\n\\n\\nAI Agents: When and How To Implement\\n\\nLLM Agent: Set Up\\n\\nReAct Architecture | Simple AI Agents\\n\\n\\nLLM Guardrails and Controls for Deployment\\n\\n\\nGuardrails\\n\\n\\nUse Cases\\n\\n\\nStructured Data Extraction\\n\\n Certifications\\n\\nLLM Certification\\nAgents, Tools, and Chains\\nLLM Evaluations\\nTraces and Spans\\nML Observability Fundamentals\\nAdvanced Model Performance Metrics\\n\\n\\nNew to Arize?\\n\\nArize onboarding\\n\\n\\n \\n\\n\\r\\n\\t\\t\\t\\t\\tThe Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems\\t\\t\\t\\t\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tAparna Dhinakaran,\\xa0\\r\\n\\t\\t\\t\\t\\t\\n\\r\\n\\t\\t\\t\\t\\t\\tCo-founder & Chief Product Officer\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\xa0|\\xa0Published February 02, 2024 \\n\\n\\nCo-authored by Evan Jolley\\nIntroduction\\nRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. With RAG’s importance likely to grow, ensuring its effectiveness is paramount.\\nThe evaluation of RAG, therefore, has become a critical part in the development and deployment of these systems. One innovative approach to this challenge is the “Needle in a Haystack” test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here.\\nWhat Is the Needle In a Haystack Test for LLMs?\\nThe “Needle In a Haystack” test is designed to evaluate the performance of LLM RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.\\nOften in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?\\nWe ran this test multiple times across several market leading language models. Let’s take a closer look at the process and overall results, first documented in this X thread.\\nWhat Are the Main Takeaways from The Needle In a Haystack Research?\\n\\nNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.\\nMinute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.\\nWhen building on top of LLMs – especially when those models are connected to private data – it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance, and in turn, customer satisfaction.\\n\\nThe Creation of The Needle In a Haystack Test\\nThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:\\nFigure 1: About 120 tokens and 50% depth\\nThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:\\nFigure 2: ChatGPT-4’s performance\\nAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the ‘needle’ is positioned towards the beginning of the context, the model tends to overlook or “forget” it, whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid. This is truly fascinating stuff.\\nFigure 3: Claude 2.1’s performance\\nAs for Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.\\nAnthropic’s Response\\nIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.\\nFirst, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, amongst an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.\\nSecond, a small edit was made to the prompt template used to query the model.\\nFigure 4: Anthropic’s Prompt Template Update\\nAs you can see, a single line was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.\\nThese changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Our team found this initial research fascinating and decided to run our own set of experiments using the Needle in a Haystack test.\\nOur Research\\nIn conducting our own series of tests, we implemented several modifications to the original experiments.The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our own evaluation library. In doing so we were able to:\\n\\nreduce the testing time from three days to just two hours, and\\nuse rails to search directly for the random number in the output, cutting through any possible wordiness that would decrease a retrieval score.\\n\\nFinally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.\\nThe updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral’s 8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, our team used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:\\nFigure 5: ChatGPT and Mixtral templating\\nAnd for Claude, we tested both previously discussed templates.\\nFigure 6: Claude templating used by Greg Kamradt \\nFigure 7: Revised Claude templating from Anthropic\\nAll code run to complete these tests can be found in this GitHub repository.\\nResults\\nFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2)\\nFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2\\nOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.\\nFigure 9: Comparison of Claude 2.1 results with and without prompting guidance\\nAs for Claude 2.1 with prompting guidance, although we were not able to replicate Anthropic’s results of 98% retrieval accuracy, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.\\n\\nAnd last but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evals.\\nConclusion\\nAs these LLMs become integral to an increasing number of products and services, our ability to evaluate and understand their retrieval capabilities will take on elevated importance.\\nThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral 8x7b MOE greatly outperformed our expectations, and we are excited to see Mistral models continually overperform expectations across our research.\\nFurther articles detailing LLM evaluation methods to follow.\\n\\n\\n\\n\\nOn this page\\n\\n\\n\\r\\n\\t\\t\\tIntroduction\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tWhat is the Needle In a Haystack Test?\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tTakeaways\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tInitial Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tAnthropic's Response\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tOur Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tResults\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tConclusion\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nTry Arize \\n\\n\\n\\n\\n\\n\\nAsk any ML question \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up for our monthly newsletter, The Drift.\\n\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlatform\\n\\n\\nSolutions\\n\\n\\nDocs\\n\\n\\nPricing\\n\\n\\n\\n\\n\\nLearn\\n\\n\\n\\nCourse\\n\\n\\nCommunity\\n\\n\\nBlog\\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\nLLM Leaderboard\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Prompt Engineering\\n\\n\\nRAG LLM\\n\\n\\nAI Orchestration\\n\\n\\nMachine Learning Observability\\n\\n\\nML Monitoring\\n\\n\\nModel Monitoring\\n\\n\\nModel Drift\\n\\n\\nKL Divergence\\n\\n\\nJensen Shannon Divergence\\n\\n\\nKolmogorov Smirnov Test\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\nCustomers\\n\\n\\nCareers\\n\\n\\nPress\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\n\\nTry now\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nContact\\n\\n\\nPrivacy Policy\\n\\n\\nTwitter\\n\\n\\nLinkedin\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 Arize AI, Inc\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\tJoin the Arize ML Observability Community\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to the Arize blog\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to get the latest news, expertise, and product updates from Arize. Your inbox is sacred, so we’ll only curate and send the best stuff.\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n*We’re committed to your privacy. Arize uses the information you provide to contact you about relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our privacy policy.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\tContact us\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tLike what you see? Let’s chat. Fill out this form and we will be in contact with you soon! \\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/', 'title': 'The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI', 'description': 'Everything you need to know about the popular technique and the importance of evaluating retrieval and model performance throughout development and deployment', 'language': 'en-US'})]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        gkamradt\\n \\n/\\n\\nLLMTest_NeedleInAHaystack\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n\\n\\n\\n \\n\\nFork\\n    61\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 656\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nLicense\\n\\n\\n\\n\\n\\n     View license\\n    \\n\\n\\n\\n\\n\\n\\n656\\n          stars\\n \\n\\n\\n\\n61\\n          forks\\n \\n\\n\\n\\nBranches\\n \\n\\n\\n\\nTags\\n \\n\\n\\n\\nActivity\\n \\n\\n\\n\\n \\n\\n\\n          Star\\n\\n  \\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n1\\n\\n\\n\\n\\n\\n\\nPull requests\\n3\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n0\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Projects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\ngkamradt/LLMTest_NeedleInAHaystack\\n\\n\\n\\n\\n\\n\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \\xa0mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History15 CommitsPaulGrahamEssaysPaulGrahamEssays\\xa0\\xa0imgimg\\xa0\\xa0original_resultsoriginal_results\\xa0\\xa0vizviz\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0Anthropic_prompt.txtAnthropic_prompt.txt\\xa0\\xa0LICENSE.txtLICENSE.txt\\xa0\\xa0LLMNeedleHaystackTester.pyLLMNeedleHaystackTester.py\\xa0\\xa0README.mdREADME.md\\xa0\\xa0requirements.txtrequirements.txt\\xa0\\xa0View all filesRepository files navigationREADMELicenseNeedle In A Haystack - Pressure Testing LLMs\\nSupported model providers: OpenAI, Anthropic\\nA simple 'needle in a haystack' analysis to test in-context retrieval ability of long context LLMs.\\nGet the behind the scenes on the overview video.\\n\\ngit clone https://github.com/gkamradt/LLMTest_NeedleInAHaystack.git\\n\\nThe Test\\n\\nPlace a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack')\\nAsk the model to retrieve this statement\\nIterate over various document depths (where the needle is placed) and context lengths to measure performance\\n\\nThis is the code that backed this OpenAI and Anthropic analysis.\\nIf ran and save_results = True, then this script will populate a result/ directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.\\nI've put the results from the original tests in /original_results. I've upgraded the script since those test were ran so the data formats may not match your script results.\\nThe key parameters:\\n\\nneedle - The statement or fact which will be placed in your context ('haystack')\\nhaystack_dir - The directory which contains the text files to load as background context. Only text files are supported\\nretrieval_question - The question with which to retrieve your needle in the background context\\nresults_version - You may want to run your test multiple times for the same combination of length/depth, change the version number if so\\ncontext_lengths_min - The starting point of your context lengths list to iterate\\ncontext_lengths_max - The ending point of your context lengths list to iterate\\ncontext_lengths_num_intervals - The number of intervals between your min/max to iterate through\\ndocument_depth_percent_min - The starting point of your document depths. Should be int > 0\\ndocument_depth_percent_max - The ending point of your document depths. Should be int < 100\\ndocument_depth_percent_intervals - The number of iterations to do between your min/max points\\ndocument_depth_percent_interval_type - Determines the distribution of depths to iterate over. 'linear' or 'sigmoid\\nmodel_provider - 'OpenAI' or 'Anthropic'\\nmodel_name - The name of the model you'd like to test. Should match the exact value which needs to be passed to the api. Ex: gpt-4-1106-preview\\nsave_results - Whether or not you'd like to save your results to file. They will be temporarily saved in the object regardless. True/False\\nsave_contexts - Whether or not you'd like to save your contexts to file. Warning these will get very long. True/False\\n\\nOther Parameters:\\n\\ncontext_lengths - A custom set of context lengths. This will override the values set for context_lengths_min, max, and intervals if set\\ndocument_depth_percents - A custom set of document depths lengths. This will override the values set for document_depth_percent_min, max, and intervals if set\\nopenai_api_key - Must be supplied. GPT-4 is used for evaluation. Can either be passed when creating the object or an environment variable\\nanthropic_api_key - Only needed if testing Anthropic models. Can either be passed when creating the object or an environment variable\\nnum_concurrent_requests - Default: 1. Set higher if you'd like to run more requests in parallel. Keep in mind rate limits.\\nfinal_context_length_buffer - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.\\nseconds_to_sleep_between_completions - Default: None, set # of seconds if you'd like to slow down your requests\\nprint_ongoing_status - Default: True, whether or not to print the status of test as they complete\\n\\nResults Visualization\\nLLMNeedleInHaystackVisualization.ipynb holds the code to make the pivot table visualization. The pivot table was then transferred to Google Slides for custom annotations and formatting. See the google slides version. See an overview of how this viz was created here.\\nOpenAI's GPT-4-128K (Run 11/8/2023)\\n\\nAnthropic's Claude 2.1 (Run 11/21/2023)\\n\\nLicense\\nThis project is licensed under the MIT License - see the LICENSE file for details. Use of this software requires attribution to the original author and project, as detailed in the license.\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nResources\\n\\n\\n\\n\\n\\n        Readme\\n \\nLicense\\n\\n\\n\\n\\n\\n     View license\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\nActivity\\n \\nStars\\n\\n\\n\\n\\n\\n656\\n      stars\\n \\nWatchers\\n\\n\\n\\n\\n\\n9\\n      watching\\n \\nForks\\n\\n\\n\\n\\n\\n61\\n      forks\\n \\n\\n\\n          Report repository\\n \\n\\n\\n\\n\\n\\n\\n\\n    Releases\\n\\nNo releases published\\n\\n\\n\\n\\n\\n\\n    Packages\\n      0\\n\\n\\n        No packages published \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Contributors\\n      2\\n\\n\\n\\n\\n\\n\\n\\n\\ngkamradt\\n\\n \\n\\n\\n\\n\\n\\n\\neltociear\\nIkko Eltociear Ashimine\\n\\n \\n\\n\\n\\n\\n\\nLanguages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJupyter Notebook\\n87.1%\\n\\n\\n\\n\\n\\n\\n\\nPython\\n12.9%\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://github.com/gkamradt/LLMTest_NeedleInAHaystack', 'title': 'GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy', 'description': 'Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack', 'language': 'en'}),\n",
       " Document(page_content='The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inImage created by author using Dall-E 3The Needle In a Haystack TestEvaluating the performance of RAG systemsAparna Dhinakaran·FollowPublished inTowards Data Science·9 min read·Feb 15, 2024--ListenShareMy thanks to Greg Kamradt and Evan Jolley for their contributions to this pieceRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses.RAG evaluation, therefore, has become a critical part in the development and deployment of these systems. One new innovative approach to this challenge is the “Needle in a Haystack’’ test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here. This test is designed to evaluate the performance of RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.Often in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?We ran this test multiple times across several major language models. Let’s take a closer look at the process and overall results.TakeawaysNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.Minute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.When building on top of LLMs — especially when those models are connected to private data — it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance.Understanding the Needle In a Haystack TestThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:Figure 1: About 120 tokens and 50% depth | Image by Greg Kamradt on X, used here with author’s permissionThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:Figure 2: ChatGPT-4’s performance | Image by Greg Kamradt on X, used here with author’s permissionAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the “needle” is positioned towards the beginning of the context, the model tends to overlook or “forget” it — whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid.Figure 3: Claude 2.1’s performance | | Image by Greg Kamradt on X, used here with author’s permissionFor Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.Anthropic’s ResponseIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.First, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, along with an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.Second, a small edit was made to the prompt template used to query the model. A single line — here is the most relevant sentence in the context — was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.PROMPT = \"\"\"HUMAN: <context>{context}</context>What is the most fun thing to do in San Francisco based on the context? Don\\'t give information outside the document or repeat our findingsAssistant: here is the most relevant sentence in the context:\"\"\"These changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Finding this initial research fascinating, we decided to run our own set of experiments using the Needle in a Haystack test.Further ExperimentsIn conducting a new series of tests, we implemented several modifications to the original experiments. The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our open source Phoenix evals library (full disclosure: I lead the team that built Phoenix) to reduce the testing time and use rails to search directly for the random number in the output, cutting through wordiness that would decrease a retrieval score. Finally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.The updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral AI’s Mixtral-8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, we used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:SIMPLE_TEMPLATE = \\'\\'\\'    You are a helpful AI bot that answers questions for a user. Keep your responses short and direct.    The following is a set of context and a question that will relate to the context.   #CONTEXT   {context}   #ENDCONTEXT   #QUESTION   {question} Don’t give information outside the document or repeat your findings. If the information is not available in the context respond UNANSWERABLEFor Claude, we tested both previously discussed templates.ANTHROPIC_TEMPLATE_ORIGINAL = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I’m going to give you the text of some essays. Amidst the essays (“the haystack”) I’ve inserted a sentence (“the needle”) that contains an answer to the user’s question. Here\\'s the question:   <question>{question}</question>   Here’s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you’ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can’t find the answer return the single word UNANSWERABLE   Assistant: \\'\\'\\'ANTHROPIC_TEMPLATE_REV2 = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I\\'m going to give you the text of some essays. Amidst the essays (\"the haystack\") I\\'ve inserted a sentence (\"the needle\") that contains an answer to the user\\'s question. Here\\'s the question:   <question>{question}</question>   Here\\'s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you\\'ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can\\'t find the answer return the single word UNANSWERABLE   Assistant: Here is the most relevant sentence in the context:\\'\\'\\'All code run to complete these tests can be found in this GitHub repository.ResultsFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2) | Image by authorFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2 | Image by authorOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.Figure 9: Comparison of Claude 2.1 results with and without prompting guidanceAlthough we were not able to replicate Anthropic’s results of 98% retrieval accuracy for Claude 2.1 with prompting guidance, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.Figure 10: Mixtral results | Image by authorLast but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evaluations.ConclusionThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral MOE greatly outperformed our expectations, and we are excited to see Mixtral models continually overperform expectations.Llm EvaluationRetrieval AugmentedMistralOpenAIHands On Tutorials----FollowWritten by Aparna Dhinakaran2.3K Followers·Writer for Towards Data ScienceCo-Founder and CPO of Arize AI. Formerly Computer Vision PhD at Cornell, Uber Machine Learning, UC Berkeley AI Research.FollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38', 'title': 'The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data Science', 'description': 'Retrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. RAG…', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNeedle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\nHomepage\\n\\n\\n\\n\\n                Leaderboard\\n              \\n\\n\\nLLM \\n\\n\\n\\nVLM \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\n\\n\\n\\n\\n\\n\\n            latest\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\nGet Started\\n\\nInstallation\\nDataset Preparation\\nQuick Start\\nFAQ\\n\\nUser Guides\\n\\nOverview\\nLearn About Config\\nConfigure Datasets\\nPrepare Models\\nEfficient Evaluation\\nTask Execution and Monitoring\\nMetric Calculation\\nResults Summary\\n\\nPrompt\\n\\nPrompt Overview\\nPrompt Template\\nMeta Template\\nChain of Thought\\n\\nAdvanced Guides\\n\\nAdd a dataset\\nCustom Dataset Tutorial\\nAdd a Model\\nEvaluation with LMDeploy\\nEvaluation with Lightllm\\nCode Evaluation Tutorial\\nCode Evaluation Docker Tutorial\\nMulti-modality Evaluation\\nPrompt Attack\\nLong Context Evaluation Guidance\\nSubjective Evaluation Guidance\\nCircularEval\\nData Contamination Assessment\\nNeedle In A Haystack Experiment Evaluation\\n\\nTools\\n\\nUseful Tools\\n\\nNotes\\n\\nContributing to OpenCompass\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Docs\\n         >\\n      \\nNeedle In A Haystack Experiment Evaluation\\n\\n 以中文阅读\\n\\n\\n\\n\\n\\n\\n\\n        Shortcuts\\n      \\n\\n\\n\\n\\n\\n\\n\\nNeedle In A Haystack Experiment Evaluation¶\\n\\nIntroduction to the Needle In A Haystack Test¶\\nThe Needle In A Haystack test, inspired by NeedleInAHaystack, is a method to evaluate the long-text information extraction ability of Large Language Models (LLMs). It involves randomly inserting key information at various points in a long text to form a prompt for LLMs. This test assesses the fundamental ability of LLMs to understand long texts by extracting critical information from them.\\n\\n\\nDataset Introduction¶\\nThe Skywork/ChineseDomainModelingEval dataset includes high-quality Chinese articles published between September and October 2023, covering multiple domains. These articles ensure a fair and challenging benchmark for testing.\\n\\n\\nFile Description¶\\nThe dataset includes files specific to various domains:\\n\\nzh_finance.jsonl - Finance\\nzh_game.jsonl - Gaming\\nzh_government.jsonl - Government\\nzh_movie.jsonl - Movies\\nzh_tech.jsonl - Technology\\nzh_general.jsonl - General\\n\\nThese files are used to assess the LLM’s understanding of different specific domains.\\n\\nEvaluation Steps¶\\n\\nDownload the dataset from Skywork/ChineseDomainModelingEval.\\nPlace the downloaded files in opencompass/data/CDME/. The expected file structure in the CDME directory is as follows:\\nopencompass/\\n├── configs\\n├── docs\\n├── data\\n│   └── CDME\\n│       ├── processed\\n│       ├── README.md\\n│       ├── zh_finance.jsonl\\n│       ├── zh_game.jsonl\\n│       ├── zh_general.jsonl\\n│       ├── zh_government.jsonl\\n│       ├── zh_movie.jsonl\\n│       └── zh_tech.jsonl\\n├── LICENSE\\n├── opencompass\\n├── outputs\\n├── run.py\\n├── more...\\n\\n\\n\\n\\n\\n\\nEnvironment Setup¶\\nconda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y\\nconda activate opencompass\\ngit clone https://github.com/open-compass/opencompass opencompass\\ncd opencompass\\npip install -e .\\n\\n\\n\\n\\nConfiguring the Dataset¶\\nIn the latest version, datasets are no longer generated by running scripts but dynamically defined and loaded through configuration files. Users need to specify dataset parameters in the configuration file according to their needs, offering greater flexibility and customization options.\\n\\nDataset Configuration Example¶\\nHere is an example of dataset configuration, showing how to define a dataset in the configs/datasets/cdme/cdme8k.py configuration file. This example demonstrates a Chinese dataset configuration with a length of 8000 tokens:\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            \\'abbr\\': f\\'CDME_Length{original_context_length}Depth{int(depth_percent)}\\',\\n            \\'type\\': CDMEDataset,\\n            \\'path\\': base_path,\\n            \\'length\\': original_context_length,\\n            \\'depth\\': int(depth_percent),\\n            \\'tokenizer_model\\': \\'gpt-4\\',\\n            \\'file_list\\': file_list,\\n            \\'num_repeats_per_file\\': 10,\\n            \\'length_buffer\\': 200,\\n            \\'guide\\': True,\\n            \\'language\\': \\'Chinese\\',\\n            \\'needle\\': \\'\\\\n小明最喜欢的实习的地点就是上海人工智能实验室。\\\\n\\',\\n            \\'retrieval_question\\': \\'小明最喜欢的实习地点是哪里？请按照“小明最喜欢的实习地点就是________。”的格式回答。\\',\\n            \\'reader_cfg\\': cdme_reader_cfg,\\n            \\'infer_cfg\\': cdme_infer_cfg,\\n            \\'eval_cfg\\': cdme_eval_cfg\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, the main parameters include:\\n\\nabbr: Abbreviation of the dataset.\\ntype: Dataset type.\\npath: Path to the dataset files.\\nlength: Context length in tokens.\\ndepth: Depth percentage of the document.\\ntokenizer_model: Tokenizer model used.\\nfile_list: List of data source files.\\nnum_repeats_per_file: Number of repeats per file.\\nlength_buffer: Length buffer.\\nguide: Whether it’s a guided dataset.\\nlanguage: Language of the dataset.\\nneedle: Specific text to find in the dataset (the ‘needle’).\\nretrieval_question: Question used to prompt the model for retrieval.\\nreader_cfg, infer_cfg, eval_cfg: Configurations for reading, inference, and evaluation, respectively.\\n\\nBy defining these parameters in the configuration file, you can flexibly create datasets that suit your needs. Configuration files offer a highly customizable and scalable way to manage the generation and use of datasets.\\n\\n\\n\\nMulti-Needle Needle In A Haystack Test¶\\nThe latest version introduces the multi-needle Needle In A Haystack test, allowing multiple different needles (text snippets) to be inserted into the same dataset. These needles are inserted in sequence according to a given depth parameter. Compared to the single-needle test, the multi-needle test provides a more complex data processing scenario.\\n\\nMulti-Needle Dataset Configuration Example¶\\nHere is an example of configuring a multi-needle dataset, showing how to define a multi-needle dataset in the configs/datasets/cdme/multi_needle/cdme8k_cot3_italy.py configuration file. This example demonstrates a dataset configuration with three needles:\\n# Basic dataset configuration\\nbase_path = \\'./data/CDME\\'\\nfile_list = [\\'zh_finance.jsonl\\']\\n\\n# Definition of Needles\\nneedles = [\\n    \\'\\\\n意大利的佛罗伦萨有一家名为\"La Giostra\"的餐馆，是整个佛罗伦萨中排行第一的餐馆。\\\\n\\',\\n    \\'\"La Giostra\"餐馆的特色菜肴是松露奶酪通心粉。\\',\\n    \\'松露奶酪通心粉是该家餐馆的有着意大利皇室烹饪血统的大厨Jack制作\\'\\n]\\n\\n\\n# Configuration parameters\\nretrieval_question = (\"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫什么？\"\\n                      \"请按照\\'制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫______。\\'的格式回答。\")\\nanswer = \"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫Jack\"\\nkeyword = \"Jack\"\\ndiff = 25\\n\\n# Dataset generation loop\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            # Other configuration items...\\n            \\'needles\\': needles,\\n            \\'diff\\': diff,\\n            \\'keyword\\': keyword,\\n            # Other configuration items...\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, in addition to the standard parameters, the main new parameters include:\\n\\nneedles: A list containing multiple strings, each representing a needle to be inserted.\\ndiff: Defines the depth increment for subsequent needles relative to the first needle.\\nkeyword: A keyword used for score correction during the evaluation process.\\n\\n\\n\\nChange in Scoring Mechanism¶\\nIn the source code of opencompass/datasets/cdme/cdme_multi.py, the scoring mechanism for multi-needle datasets differs. The following code segment has been added to adjust the scores based on the keyword in the predictions:\\nif keyword in prediction:\\n    print(f\\'{keyword} is in {prediction}\\')\\n    score = 100\\nelse:\\n    print(f\\'{keyword} is not in {prediction}\\')\\n    score = 0.2 * score\\n\\n\\nThis code means that if the keyword is present in the prediction, it will be awarded a high score (e.g., 100). If not, the score will be significantly reduced (20% of the original score). This scoring mechanism places more emphasis on the accuracy of keywords, supplementing the traditional scoring methods.\\n\\n\\n\\nEvaluation¶\\n\\nEvaluating with the internlm Model¶\\nFor example, to evaluate using the internlm model, the following command can be used:\\npython run.py configs/eval_needleinahaystack.py --slurm -p partition_name -q auto --max-num-workers 32\\n\\n\\nThis command initiates the evaluation process, where the model attempts to find the specified “needle” in the generated dataset. The parameters -p partition_name -q auto and --max-num-workers 32 specify the Slurm queue and the maximum number of worker processes, respectively.\\n\\n\\nLarge-Scale Text Evaluation with LMDeploy¶\\nWhen evaluating especially long texts (e.g., 200k tokens), conventional methods might lead to memory overload. In such cases, quantized models can be used for evaluation. This can be achieved using the LMDeploy tool (LMDeploy).\\nDetailed information about installing and configuring LMDeploy can be found on its GitHub page. Once installed, the TurboMindModel defined in the configs/eval_needleinahaystack_turbomind.py configuration file can be used for evaluation.\\nBelow is an example configuration in the configs/eval_needleinahaystack_turbomind.py file:\\nfrom opencompass.models.turbomind import TurboMindModel\\nfrom mmengine.config import read_base\\n\\nwith read_base():\\n    from .datasets.cdme.cdme200k import cdme_datasets\\n\\ndatasets = [*cdme_datasets]\\n\\ninternlm_meta_template = dict(round=[\\n    dict(role=\\'HUMAN\\', begin=\\':\\', end=\\'\\\\n\\'),\\n    dict(role=\\'BOT\\', begin=\\':\\', end=\\'<eoa>\\\\n\\', generate=True),\\n],\\n                              eos_token_id=103028)\\n\\nmodels = [\\n    dict(\\n        type=TurboMindModel,\\n        abbr=\\'internlm-chat-20b-turbomind\\',\\n        path=\\'./turbomind\\',\\n        max_out_len=100,\\n        max_seq_len=2048,\\n        batch_size=8,\\n        concurrency=8,\\n        meta_template=internlm_meta_template,\\n        run_cfg=dict(num_gpus=1, num_procs=1),\\n    )\\n]\\n\\n\\nIn this configuration, the TurboMindModel combines the functionality of LMDeploy, suitable for handling large-scale text datasets and effectively reducing memory usage.\\n\\n\\n\\nScore Calculation Method¶\\nIn the CDMEEvaluator class, we use two main methods to calculate scores: levenshtein_distance and score. Here are detailed explanations and implementations of these methods.\\n\\nLevenshtein Distance¶\\nLevenshtein distance is a measure of the difference between two strings. It represents the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\\ndef levenshtein_distance(self, s1, s2):\\n    if len(s1) < len(s2):\\n        return self.levenshtein_distance(s2, s1)\\n\\n    if len(s2) == 0:\\n        return len(s1)\\n\\n    previous_row = range(len(s2) + 1)\\n    for i, c1 in enumerate(s1):\\n        current_row = [i + 1]\\n        for j, c2 in enumerate(s2):\\n            insertions = previous_row[j + 1] + 1\\n            deletions = current_row[j] + 1\\n            substitutions = previous_row[j] + (c1 != c2)\\n            current_row.append(min(insertions, deletions, substitutions))\\n        previous_row = current_row\\n\\n    return previous_row[-1]\\n\\n\\n\\n\\nScore Calculation¶\\nThe score calculation method accepts two lists of predictions and references and calculates the edit distance and score for each pair of prediction and reference.\\ndef score(self, predictions, references):\\n    if len(predictions) != len(references):\\n        return {\"error\": \"predictions and references have different lengths\"}\\n\\n    total_score = 0\\n    details = []\\n    for prediction, reference in zip(predictions, references):\\n        prediction = re.sub(r\\'\\\\s+\\', \\'\\', prediction)\\n        reference = re.sub(r\\'\\\\s+\\', \\'\\', reference)\\n        edit_distance = self.levenshtein_distance(prediction, reference)\\n\\n\\n        max_len = max(len(prediction), len(reference))\\n        score = 100 * (1 - edit_distance /max_len) if max_len != 0 else 100\\n\\n        detail = {\\n            \"pred\": prediction,\\n            \"ref\": reference,\\n            \"edit_distance\": edit_distance,\\n            \"score\": score\\n        }\\n        total_score += score\\n        details.append(detail)\\n\\n    average_score = total_score / len(predictions) if predictions else 0\\n    result = {\"average_score\": average_score, \"details\": details}\\n    return result\\n\\n\\nThis scoring method first removes all whitespace characters from both predictions and references and then calculates the Levenshtein distance between them. The score is calculated as 100 minus the percentage loss based on edit distance. Finally, it returns detailed scores for each prediction and the average score overall.\\n\\n\\n\\nVisualization¶\\nThe tools_needleinahaystack.py script can be used to visualize CSV files. This script supports specifying one or more CSV file paths through the --path parameter and can use the --dataset_length parameter to specify the length of the dataset.\\n\\nUsage Examples¶\\nTo visualize a single CSV file:\\npython tools/tools_needleinahaystack.py --path \\'outputs/default/20231216_161457/summary/summary_20231216_161457.csv\\'\\n\\n\\nTo visualize multiple CSV files:\\npython tools/tools_needleinahaystack.py --path \\'path_to_first_csv.csv\\' \\'path_to_second_csv.csv\\'\\n\\n\\nTo specify the dataset length for visualization, which is used for generating titles in the visualization charts:\\npython tools/tools_needleinahaystack.py --path \\'path_to_csv.csv\\' --dataset_length 200K\\n\\n\\nCurrently, this approach only supports the CDME dataset, and we welcome community contributions for more datasets.\\nIf you use this method, please cite as follows:\\n@misc{2023opencompass,\\n    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},\\n    author={OpenCompass Contributors},\\n    howpublished={\\\\url{https://github.com/open-compass/opencompass}},\\n    year={2023}\\n}\\n\\n@misc{LLMTest_NeedleInAHaystack,\\n  title={LLMTest Needle In A Haystack - Pressure Testing LLMs},\\n  author={gkamradt},\\n  year={2023},\\n  howpublished={\\\\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}}\\n}\\n\\n@misc{wei2023skywork,\\n      title={Skywork: A More Open Bilingual Foundation Model},\\n      author={Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei Lü and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and Xuejie Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},\\n      year={2023},\\n      eprint={2310.19341},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNext \\n Previous\\n\\n\\n\\n\\n      © Copyright 2023, OpenCompass.\\n      \\n      \\n        Revision 120bf8b3.\\n      \\n\\n\\n\\n    Built with Sphinx using a theme provided by Read the\\n      Docs.\\n  \\n\\n\\n\\n\\n\\n\\n\\nNeedle In A Haystack Experiment Evaluation\\nIntroduction to the Needle In A Haystack Test\\nDataset Introduction\\nFile Description\\nEvaluation Steps\\nEnvironment Setup\\nConfiguring the Dataset\\nDataset Configuration Example\\n\\n\\nMulti-Needle Needle In A Haystack Test\\nMulti-Needle Dataset Configuration Example\\nChange in Scoring Mechanism\\n\\n\\nEvaluation\\nEvaluating with the internlm Model\\nLarge-Scale Text Evaluation with LMDeploy\\n\\n\\nScore Calculation Method\\nLevenshtein Distance\\nScore Calculation\\n\\n\\nVisualization\\nUsage Examples\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Read the Docs\\n      v: latest\\n      \\n\\n\\n\\nVersions\\nlatest\\nstable\\n\\n\\nDownloads\\nepub\\n\\n\\nOn Read the Docs\\n\\nProject Home\\n\\n\\nBuilds\\n\\n\\n\\n      Free document hosting provided by Read the Docs.\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n@沪ICP备2021009351号-23\\nOpenCompass Open Platform Service Agreement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html', 'title': 'Needle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation', 'language': 'en'}),\n",
       " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\nThe Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFree LLM Observability Courses |\\xa0Enroll now!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlatform\\n\\nOverview\\nLeft ColumnMonitorsDashboardsEval & Performance TracingExplainability & Fairness\\nRight ColumnEmbeddings & RAG AnalyzerLLM TracingFine TunePhoenix OSS\\nArize Product Demo: See the Platform in ActionWatch now\\n\\n\\nSolutions\\n\\nOverview\\nLeft ColumnUse CasesComputer VisionRecommender SystemsRegression & ClassificationForecasting\\nRight ColumnIndustriesFinancial ServiceseCommerceMedia & EntertainmentAutonomous VehiclesManufacturingBiotechnology & Pharmaceutical Research\\nCustomersLearn more\\n\\n\\nPricing\\n\\n\\nLearn\\n\\nLeft ColumnResourcesBlogPodcastEventsVideosPaper Readings\\nRight ColumnArize UniversityCertificationBasic and advanced course certificationLLMOpsSelf-guided LLMops learningML ObservabilitySelf-guided observability learning\\nArize CommunityGreat discussions, support, and random acts of swag await!\\nJoin now\\n\\n\\nDocs\\n\\n\\nCompany\\n\\nAbout\\nPartners\\nCareers\\nPress\\nSecurity\\nArize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nSign Up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n×\\nARIZE GENERATIVE AI COURSE\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tIntro To LLMOps\\t\\t\\n\\nIntro To LLMOps and LLM Observability\\n\\n\\nWhat is LLM Observability?\\n\\nIntroduction To LLMOps\\n\\nFoundation Models\\n\\nOpenAI API\\n\\nLLM Deployment\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Evaluation: Build and Benchmark Evals\\n\\nThe Needle In a Haystack Test\\n\\nEvals from OpenAI\\n\\n\\nTraces and Spans\\n\\n\\nTraces and Spans in Orchestration Frameworks\\n\\n\\nSearch and Retrieval: Build Your Own Chatbot\\n\\n\\nRetrieval Augmented Generation (RAG)\\n\\nBenchmarking Evaluation of LLM RAG\\n\\nRAG Evaluation\\n\\n\\nPrompt Engineering\\n\\n\\nEvaluating Prompts: A Developer’s Guide\\n\\n\\nChains and Agents\\n\\n\\nAI Agents: When and How To Implement\\n\\nLLM Agent: Set Up\\n\\nReAct Architecture | Simple AI Agents\\n\\n\\nLLM Guardrails and Controls for Deployment\\n\\n\\nGuardrails\\n\\n\\nUse Cases\\n\\n\\nStructured Data Extraction\\n\\n Certifications\\n\\nLLM Certification\\nAgents, Tools, and Chains\\nLLM Evaluations\\nTraces and Spans\\nML Observability Fundamentals\\nAdvanced Model Performance Metrics\\n\\n\\nNew to Arize?\\n\\nArize onboarding\\n\\n\\n \\n\\n\\r\\n\\t\\t\\t\\t\\tThe Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems\\t\\t\\t\\t\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tAparna Dhinakaran,\\xa0\\r\\n\\t\\t\\t\\t\\t\\n\\r\\n\\t\\t\\t\\t\\t\\tCo-founder & Chief Product Officer\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\xa0|\\xa0Published February 02, 2024 \\n\\n\\nCo-authored by Evan Jolley\\nIntroduction\\nRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. With RAG’s importance likely to grow, ensuring its effectiveness is paramount.\\nThe evaluation of RAG, therefore, has become a critical part in the development and deployment of these systems. One innovative approach to this challenge is the “Needle in a Haystack” test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here.\\nWhat Is the Needle In a Haystack Test for LLMs?\\nThe “Needle In a Haystack” test is designed to evaluate the performance of LLM RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.\\nOften in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?\\nWe ran this test multiple times across several market leading language models. Let’s take a closer look at the process and overall results, first documented in this X thread.\\nWhat Are the Main Takeaways from The Needle In a Haystack Research?\\n\\nNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.\\nMinute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.\\nWhen building on top of LLMs – especially when those models are connected to private data – it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance, and in turn, customer satisfaction.\\n\\nThe Creation of The Needle In a Haystack Test\\nThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:\\nFigure 1: About 120 tokens and 50% depth\\nThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:\\nFigure 2: ChatGPT-4’s performance\\nAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the ‘needle’ is positioned towards the beginning of the context, the model tends to overlook or “forget” it, whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid. This is truly fascinating stuff.\\nFigure 3: Claude 2.1’s performance\\nAs for Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.\\nAnthropic’s Response\\nIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.\\nFirst, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, amongst an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.\\nSecond, a small edit was made to the prompt template used to query the model.\\nFigure 4: Anthropic’s Prompt Template Update\\nAs you can see, a single line was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.\\nThese changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Our team found this initial research fascinating and decided to run our own set of experiments using the Needle in a Haystack test.\\nOur Research\\nIn conducting our own series of tests, we implemented several modifications to the original experiments.The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our own evaluation library. In doing so we were able to:\\n\\nreduce the testing time from three days to just two hours, and\\nuse rails to search directly for the random number in the output, cutting through any possible wordiness that would decrease a retrieval score.\\n\\nFinally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.\\nThe updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral’s 8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, our team used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:\\nFigure 5: ChatGPT and Mixtral templating\\nAnd for Claude, we tested both previously discussed templates.\\nFigure 6: Claude templating used by Greg Kamradt \\nFigure 7: Revised Claude templating from Anthropic\\nAll code run to complete these tests can be found in this GitHub repository.\\nResults\\nFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2)\\nFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2\\nOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.\\nFigure 9: Comparison of Claude 2.1 results with and without prompting guidance\\nAs for Claude 2.1 with prompting guidance, although we were not able to replicate Anthropic’s results of 98% retrieval accuracy, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.\\n\\nAnd last but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evals.\\nConclusion\\nAs these LLMs become integral to an increasing number of products and services, our ability to evaluate and understand their retrieval capabilities will take on elevated importance.\\nThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral 8x7b MOE greatly outperformed our expectations, and we are excited to see Mistral models continually overperform expectations across our research.\\nFurther articles detailing LLM evaluation methods to follow.\\n\\n\\n\\n\\nOn this page\\n\\n\\n\\r\\n\\t\\t\\tIntroduction\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tWhat is the Needle In a Haystack Test?\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tTakeaways\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tInitial Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tAnthropic's Response\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tOur Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tResults\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tConclusion\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nTry Arize \\n\\n\\n\\n\\n\\n\\nAsk any ML question \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up for our monthly newsletter, The Drift.\\n\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlatform\\n\\n\\nSolutions\\n\\n\\nDocs\\n\\n\\nPricing\\n\\n\\n\\n\\n\\nLearn\\n\\n\\n\\nCourse\\n\\n\\nCommunity\\n\\n\\nBlog\\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\nLLM Leaderboard\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Prompt Engineering\\n\\n\\nRAG LLM\\n\\n\\nAI Orchestration\\n\\n\\nMachine Learning Observability\\n\\n\\nML Monitoring\\n\\n\\nModel Monitoring\\n\\n\\nModel Drift\\n\\n\\nKL Divergence\\n\\n\\nJensen Shannon Divergence\\n\\n\\nKolmogorov Smirnov Test\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\nCustomers\\n\\n\\nCareers\\n\\n\\nPress\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\n\\nTry now\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nContact\\n\\n\\nPrivacy Policy\\n\\n\\nTwitter\\n\\n\\nLinkedin\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 Arize AI, Inc\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\tJoin the Arize ML Observability Community\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to the Arize blog\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to get the latest news, expertise, and product updates from Arize. Your inbox is sacred, so we’ll only curate and send the best stuff.\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n*We’re committed to your privacy. Arize uses the information you provide to contact you about relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our privacy policy.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n\\t\\t\\t\\tContact us\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tLike what you see? Let’s chat. Fill out this form and we will be in contact with you soon! \\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/', 'title': 'The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI', 'description': 'Everything you need to know about the popular technique and the importance of evaluating retrieval and model performance throughout development and deployment', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "docs_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Text splitter that uses tiktoken encoder to count length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "tiktoken is a fast BPE tokenizer created by OpenAI.\n",
    "\n",
    "Byte-Pair Encoding: Subword-based tokenization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n\\nSkip to content\\n\\nToggle navigation\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n        Product\\n        \\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\nPackages\\n        Host and manage packages\\n      \\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\nCopilot\\n        Write better code with AI\\n      \\n\\nCode review\\n        Manage code changes\\n      \\n\\nIssues\\n        Plan and track work\\n      \\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\nExplore\\n\\n      All features\\n\\n    \\n\\n      Documentation\\n\\n    \\n\\n      GitHub Skills\\n\\n    \\n\\n      Blog\\n\\n    \\n\\n        Solutions\\n        \\n\\nFor\\n\\n      Enterprise\\n\\n    \\n\\n      Teams\\n\\n    \\n\\n      Startups\\n\\n    \\n\\n      Education\\n\\n    \\n\\n\\nBy Solution\\n\\n      CI/CD & Automation\\n\\n    \\n\\n      DevOps\\n\\n    \\n\\n      DevSecOps\\n\\n    \\n\\n\\nResources\\n\\n      Learning Pathways\\n\\n    \\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n      Customer Stories\\n\\n    \\n\\n      Partners\\n\\n    \\n\\n        Open Source\\n        \\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\nRepositories\\n\\n      Topics\\n\\n    \\n\\n      Trending\\n\\n    \\n\\n      Collections\\n\\n    \\n\\n\\nPricing\\n\\n\\nSearch or jump to...\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n        Search\\n      \\n\\nClear\\n \\n\\n \\n\\n\\n              Search syntax tips\\n \\n\\n\\n        Provide feedback\\n      \\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n \\n\\nName\\n\\n\\nQuery\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n        gkamradt\\n \\n/\\n\\nLLMTest_NeedleInAHaystack\\n\\nPublic\\n\\n \\n\\nNotifications\\n\\n \\n\\nFork\\n    61\\n\\n\\n \\n\\n\\n          Star\\n 656\\n  \\n\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nLicense\\n\\n     View license\\n    \\n\\n\\n656\\n          stars\\n \\n\\n61\\n          forks\\n \\n\\nBranches\\n \\n\\nTags\\n \\n\\nActivity\\n \\n\\n \\n\\n\\n          Star\\n\\n  \\n\\n \\n\\nNotifications\\n\\n\\nCode\\n\\nIssues\\n1\\n\\n\\nPull requests\\n3\\n\\n\\nActions\\n\\nProjects\\n0\\n\\n\\nSecurity\\n\\nInsights\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n          Code\\n\\n\\n          Issues\\n\\n\\n          Pull requests\\n\\n\\n          Actions\\n\\n\\n          Projects\\n\\n\\n          Security\\n\\n\\n          Insights\\n\\n \\n\\ngkamradt/LLMTest_NeedleInAHaystack\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\n\\n   \\xa0mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History15 CommitsPaulGrahamEssaysPaulGrahamEssays\\xa0\\xa0imgimg\\xa0\\xa0original_resultsoriginal_results\\xa0\\xa0vizviz\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0Anthropic_prompt.txtAnthropic_prompt.txt\\xa0\\xa0LICENSE.txtLICENSE.txt\\xa0\\xa0LLMNeedleHaystackTester.pyLLMNeedleHaystackTester.py\\xa0\\xa0README.mdREADME.md\\xa0\\xa0requirements.txtrequirements.txt\\xa0\\xa0View all filesRepository files navigationREADMELicenseNeedle In A Haystack - Pressure Testing LLMs\\nSupported model providers: OpenAI, Anthropic\\nA simple 'needle in a haystack' analysis to test in-context retrieval ability of long context LLMs.\\nGet the behind the scenes on the overview video.\\n\\ngit clone https://github.com/gkamradt/LLMTest_NeedleInAHaystack.git\\n\\nThe Test\\n\\nPlace a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack')\\nAsk the model to retrieve this statement\\nIterate over various document depths (where the needle is placed) and context lengths to measure performance\\n\\nThis is the code that backed this OpenAI and Anthropic analysis.\\nIf ran and save_results = True, then this script will populate a result/ directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.\\nI've put the results from the original tests in /original_results. I've upgraded the script since those test were ran so the data formats may not match your script results.\\nThe key parameters:\\n\\nneedle - The statement or fact which will be placed in your context ('haystack')\\nhaystack_dir - The directory which contains the text files to load as background context. Only text files are supported\\nretrieval_question - The question with which to retrieve your needle in the background context\\nresults_version - You may want to run your test multiple times for the same combination of length/depth, change the version number if so\\ncontext_lengths_min - The starting point of your context lengths list to iterate\\ncontext_lengths_max - The ending point of your context lengths list to iterate\\ncontext_lengths_num_intervals - The number of intervals between your min/max to iterate through\\ndocument_depth_percent_min - The starting point of your document depths. Should be int > 0\\ndocument_depth_percent_max - The ending point of your document depths. Should be int < 100\\ndocument_depth_percent_intervals - The number of iterations to do between your min/max points\\ndocument_depth_percent_interval_type - Determines the distribution of depths to iterate over. 'linear' or 'sigmoid\\nmodel_provider - 'OpenAI' or 'Anthropic'\\nmodel_name - The name of the model you'd like to test. Should match the exact value which needs to be passed to the api. Ex: gpt-4-1106-preview\\nsave_results - Whether or not you'd like to save your results to file. They will be temporarily saved in the object regardless. True/False\\nsave_contexts - Whether or not you'd like to save your contexts to file. Warning these will get very long. True/False\\n\\nOther Parameters:\\n\\ncontext_lengths - A custom set of context lengths. This will override the values set for context_lengths_min, max, and intervals if set\\ndocument_depth_percents - A custom set of document depths lengths. This will override the values set for document_depth_percent_min, max, and intervals if set\\nopenai_api_key - Must be supplied. GPT-4 is used for evaluation. Can either be passed when creating the object or an environment variable\\nanthropic_api_key - Only needed if testing Anthropic models. Can either be passed when creating the object or an environment variable\\nnum_concurrent_requests - Default: 1. Set higher if you'd like to run more requests in parallel. Keep in mind rate limits.\\nfinal_context_length_buffer - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.\\nseconds_to_sleep_between_completions - Default: None, set # of seconds if you'd like to slow down your requests\\nprint_ongoing_status - Default: True, whether or not to print the status of test as they complete\\n\\nResults Visualization\\nLLMNeedleInHaystackVisualization.ipynb holds the code to make the pivot table visualization. The pivot table was then transferred to Google Slides for custom annotations and formatting. See the google slides version. See an overview of how this viz was created here.\\nOpenAI's GPT-4-128K (Run 11/8/2023)\\n\\nAnthropic's Claude 2.1 (Run 11/21/2023)\\n\\nLicense\\nThis project is licensed under the MIT License - see the LICENSE file for details. Use of this software requires attribution to the original author and project, as detailed in the license.\\n   \\n\\n\\nAbout\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nResources\\n\\n        Readme\\n \\nLicense\\n\\n     View license\\n    \\n\\n\\nActivity\\n \\nStars\\n\\n656\\n      stars\\n \\nWatchers\\n\\n9\\n      watching\\n \\nForks\\n\\n61\\n      forks\\n \\n\\n\\n          Report repository\\n \\n\\n    Releases\\n\\nNo releases published\\n\\n\\n    Packages\\n      0\\n\\n\\n        No packages published \\n\\n    Contributors\\n      2\\n\\n\\ngkamradt\\n\\n \\n\\n\\neltociear\\nIkko Eltociear Ashimine\\n\\n \\n\\nLanguages\\n\\nJupyter Notebook\\n87.1%\\n\\nPython\\n12.9%\\n\\nFooter\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n      Manage cookies\\n    \\n\\n      Do not share my personal information\\n    \\n\\n\\n    You can’t perform that action at this time.\", metadata={'source': 'https://github.com/gkamradt/LLMTest_NeedleInAHaystack', 'title': 'GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy', 'description': 'Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack', 'language': 'en'}),\n",
       " Document(page_content='The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inImage created by author using Dall-E 3The Needle In a Haystack TestEvaluating the performance of RAG systemsAparna Dhinakaran·FollowPublished inTowards Data Science·9 min read·Feb 15, 2024--ListenShareMy thanks to Greg Kamradt and Evan Jolley for their contributions to this pieceRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses.RAG evaluation, therefore, has become a critical part in the development and deployment of these systems. One new innovative approach to this challenge is the “Needle in a Haystack’’ test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here. This test is designed to evaluate the performance of RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.Often in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?We ran this test multiple times across several major language models. Let’s take a closer look at the process and overall results.TakeawaysNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.Minute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.When building on top of LLMs — especially when those models are connected to private data — it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance.Understanding the Needle In a Haystack TestThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:Figure 1: About 120 tokens and 50% depth | Image by Greg Kamradt on X, used here with author’s permissionThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:Figure 2: ChatGPT-4’s performance | Image by Greg Kamradt on X, used here with author’s permissionAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the “needle” is positioned towards the beginning of the context, the model tends to overlook or “forget” it — whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid.Figure 3: Claude 2.1’s performance | | Image by Greg Kamradt on X, used here with author’s permissionFor Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.Anthropic’s ResponseIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.First, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, along with an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.Second, a small edit was made to the prompt template used to query the model. A single line — here is the most relevant sentence in the context — was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.PROMPT = \"\"\"HUMAN: <context>{context}</context>What is the most fun thing to do in San Francisco based on the context? Don\\'t give information outside the document or repeat our findingsAssistant: here is the most relevant sentence in the context:\"\"\"These changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Finding this initial research fascinating, we decided to run our own set of experiments using the Needle in a Haystack test.Further ExperimentsIn conducting a new series of tests, we implemented several modifications to the original experiments. The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our open source Phoenix evals library (full disclosure: I lead the team that built Phoenix) to reduce the testing time and use rails to search directly for the random number in the output, cutting through wordiness that would decrease a retrieval score. Finally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.The updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral AI’s Mixtral-8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, we used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:SIMPLE_TEMPLATE = \\'\\'\\'    You are a helpful AI bot that answers questions for a user. Keep your responses short and direct.    The following is a set of context and a question that will relate to the context.   #CONTEXT   {context}   #ENDCONTEXT   #QUESTION   {question} Don’t give information outside the document or repeat your findings. If the information is not available in the context respond UNANSWERABLEFor Claude, we tested both previously discussed templates.ANTHROPIC_TEMPLATE_ORIGINAL = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I’m going to give you the text of some essays. Amidst the essays (“the haystack”) I’ve inserted a sentence (“the needle”) that contains an answer to the user’s question. Here\\'s the question:   <question>{question}</question>   Here’s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you’ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can’t find the answer return the single word UNANSWERABLE   Assistant: \\'\\'\\'ANTHROPIC_TEMPLATE_REV2 = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I\\'m going to give you the text of some essays. Amidst the essays (\"the haystack\") I\\'ve inserted a sentence (\"the needle\") that contains an answer to the user\\'s question. Here\\'s the question:   <question>{question}</question>   Here\\'s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you\\'ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can\\'t find the answer return the single word UNANSWERABLE   Assistant: Here is the most relevant sentence in the context:\\'\\'\\'All code run to complete these tests can be found in this GitHub repository.ResultsFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2) | Image by authorFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2 | Image by authorOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.Figure 9: Comparison of Claude 2.1 results with and without prompting guidanceAlthough we were not able to replicate Anthropic’s results of 98% retrieval accuracy for Claude 2.1 with prompting guidance, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.Figure 10: Mixtral results | Image by authorLast but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evaluations.ConclusionThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral MOE greatly outperformed our expectations, and we are excited to see Mixtral models continually overperform expectations.Llm EvaluationRetrieval AugmentedMistralOpenAIHands On Tutorials----FollowWritten by Aparna Dhinakaran2.3K Followers·Writer for Towards Data ScienceCo-Founder and CPO of Arize AI. Formerly Computer Vision PhD at Cornell, Uber Machine Learning, UC Berkeley AI Research.FollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams', metadata={'source': 'https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38', 'title': 'The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data Science', 'description': 'Retrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. RAG…', 'language': 'en'}),\n",
       " Document(page_content='Needle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation\\n\\nGitHub\\n\\n\\nHomepage\\n\\n\\n                Leaderboard\\n              \\n\\n\\nLLM \\n\\nVLM \\n\\n\\nTable of Contents\\n\\n            latest\\n          \\n\\n\\nGet Started\\n\\nInstallation\\nDataset Preparation\\nQuick Start\\nFAQ\\n\\nUser Guides\\n\\nOverview\\nLearn About Config\\nConfigure Datasets\\nPrepare Models\\nEfficient Evaluation\\nTask Execution and Monitoring\\nMetric Calculation\\nResults Summary\\n\\nPrompt\\n\\nPrompt Overview\\nPrompt Template\\nMeta Template\\nChain of Thought\\n\\nAdvanced Guides\\n\\nAdd a dataset\\nCustom Dataset Tutorial\\nAdd a Model\\nEvaluation with LMDeploy\\nEvaluation with Lightllm\\nCode Evaluation Tutorial\\nCode Evaluation Docker Tutorial\\nMulti-modality Evaluation\\nPrompt Attack\\nLong Context Evaluation Guidance\\nSubjective Evaluation Guidance\\nCircularEval\\nData Contamination Assessment\\nNeedle In A Haystack Experiment Evaluation\\n\\nTools\\n\\nUseful Tools\\n\\nNotes\\n\\nContributing to OpenCompass\\n\\n            Docs\\n         >\\n      \\nNeedle In A Haystack Experiment Evaluation\\n\\n 以中文阅读\\n\\n        Shortcuts\\n      \\n\\nNeedle In A Haystack Experiment Evaluation¶\\n\\nIntroduction to the Needle In A Haystack Test¶\\nThe Needle In A Haystack test, inspired by NeedleInAHaystack, is a method to evaluate the long-text information extraction ability of Large Language Models (LLMs). It involves randomly inserting key information at various points in a long text to form a prompt for LLMs. This test assesses the fundamental ability of LLMs to understand long texts by extracting critical information from them.\\n\\n\\nDataset Introduction¶\\nThe Skywork/ChineseDomainModelingEval dataset includes high-quality Chinese articles published between September and October 2023, covering multiple domains. These articles ensure a fair and challenging benchmark for testing.\\n\\n\\nFile Description¶\\nThe dataset includes files specific to various domains:\\n\\nzh_finance.jsonl - Finance\\nzh_game.jsonl - Gaming\\nzh_government.jsonl - Government\\nzh_movie.jsonl - Movies\\nzh_tech.jsonl - Technology\\nzh_general.jsonl - General\\n\\nThese files are used to assess the LLM’s understanding of different specific domains.\\n\\nEvaluation Steps¶\\n\\nDownload the dataset from Skywork/ChineseDomainModelingEval.\\nPlace the downloaded files in opencompass/data/CDME/. The expected file structure in the CDME directory is as follows:\\nopencompass/\\n├── configs\\n├── docs\\n├── data\\n│   └── CDME\\n│       ├── processed\\n│       ├── README.md\\n│       ├── zh_finance.jsonl\\n│       ├── zh_game.jsonl\\n│       ├── zh_general.jsonl\\n│       ├── zh_government.jsonl\\n│       ├── zh_movie.jsonl\\n│       └── zh_tech.jsonl\\n├── LICENSE\\n├── opencompass\\n├── outputs\\n├── run.py\\n├── more...\\n\\n\\nEnvironment Setup¶\\nconda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y\\nconda activate opencompass\\ngit clone https://github.com/open-compass/opencompass opencompass\\ncd opencompass\\npip install -e .\\n\\n\\nConfiguring the Dataset¶\\nIn the latest version, datasets are no longer generated by running scripts but dynamically defined and loaded through configuration files. Users need to specify dataset parameters in the configuration file according to their needs, offering greater flexibility and customization options.\\n\\nDataset Configuration Example¶\\nHere is an example of dataset configuration, showing how to define a dataset in the configs/datasets/cdme/cdme8k.py configuration file. This example demonstrates a Chinese dataset configuration with a length of 8000 tokens:\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            \\'abbr\\': f\\'CDME_Length{original_context_length}Depth{int(depth_percent)}\\',\\n            \\'type\\': CDMEDataset,\\n            \\'path\\': base_path,\\n            \\'length\\': original_context_length,\\n            \\'depth\\': int(depth_percent),\\n            \\'tokenizer_model\\': \\'gpt-4\\',\\n            \\'file_list\\': file_list,\\n            \\'num_repeats_per_file\\': 10,\\n            \\'length_buffer\\': 200,\\n            \\'guide\\': True,\\n            \\'language\\': \\'Chinese\\',\\n            \\'needle\\': \\'\\\\n小明最喜欢的实习的地点就是上海人工智能实验室。\\\\n\\',\\n            \\'retrieval_question\\': \\'小明最喜欢的实习地点是哪里？请按照“小明最喜欢的实习地点就是________。”的格式回答。\\',\\n            \\'reader_cfg\\': cdme_reader_cfg,\\n            \\'infer_cfg\\': cdme_infer_cfg,\\n            \\'eval_cfg\\': cdme_eval_cfg\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, the main parameters include:\\n\\nabbr: Abbreviation of the dataset.\\ntype: Dataset type.\\npath: Path to the dataset files.\\nlength: Context length in tokens.\\ndepth: Depth percentage of the document.\\ntokenizer_model: Tokenizer model used.\\nfile_list: List of data source files.\\nnum_repeats_per_file: Number of repeats per file.\\nlength_buffer: Length buffer.\\nguide: Whether it’s a guided dataset.\\nlanguage: Language of the dataset.\\nneedle: Specific text to find in the dataset (the ‘needle’).\\nretrieval_question: Question used to prompt the model for retrieval.\\nreader_cfg, infer_cfg, eval_cfg: Configurations for reading, inference, and evaluation, respectively.\\n\\nBy defining these parameters in the configuration file, you can flexibly create datasets that suit your needs. Configuration files offer a highly customizable and scalable way to manage the generation and use of datasets.\\n\\nMulti-Needle Needle In A Haystack Test¶\\nThe latest version introduces the multi-needle Needle In A Haystack test, allowing multiple different needles (text snippets) to be inserted into the same dataset. These needles are inserted in sequence according to a given depth parameter. Compared to the single-needle test, the multi-needle test provides a more complex data processing scenario.\\n\\nMulti-Needle Dataset Configuration Example¶\\nHere is an example of configuring a multi-needle dataset, showing how to define a multi-needle dataset in the configs/datasets/cdme/multi_needle/cdme8k_cot3_italy.py configuration file. This example demonstrates a dataset configuration with three needles:\\n# Basic dataset configuration\\nbase_path = \\'./data/CDME\\'\\nfile_list = [\\'zh_finance.jsonl\\']\\n\\n# Definition of Needles\\nneedles = [\\n    \\'\\\\n意大利的佛罗伦萨有一家名为\"La Giostra\"的餐馆，是整个佛罗伦萨中排行第一的餐馆。\\\\n\\',\\n    \\'\"La Giostra\"餐馆的特色菜肴是松露奶酪通心粉。\\',\\n    \\'松露奶酪通心粉是该家餐馆的有着意大利皇室烹饪血统的大厨Jack制作\\'\\n]\\n\\n\\n# Configuration parameters\\nretrieval_question = (\"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫什么？\"\\n                      \"请按照\\'制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫______。\\'的格式回答。\")\\nanswer = \"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫Jack\"\\nkeyword = \"Jack\"\\ndiff = 25\\n\\n# Dataset generation loop\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            # Other configuration items...\\n            \\'needles\\': needles,\\n            \\'diff\\': diff,\\n            \\'keyword\\': keyword,\\n            # Other configuration items...\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, in addition to the standard parameters, the main new parameters include:\\n\\nneedles: A list containing multiple strings, each representing a needle to be inserted.\\ndiff: Defines the depth increment for subsequent needles relative to the first needle.\\nkeyword: A keyword used for score correction during the evaluation process.\\n\\nChange in Scoring Mechanism¶\\nIn the source code of opencompass/datasets/cdme/cdme_multi.py, the scoring mechanism for multi-needle datasets differs. The following code segment has been added to adjust the scores based on the keyword in the predictions:\\nif keyword in prediction:\\n    print(f\\'{keyword} is in {prediction}\\')\\n    score = 100\\nelse:\\n    print(f\\'{keyword} is not in {prediction}\\')\\n    score = 0.2 * score\\n\\n\\nThis code means that if the keyword is present in the prediction, it will be awarded a high score (e.g., 100). If not, the score will be significantly reduced (20% of the original score). This scoring mechanism places more emphasis on the accuracy of keywords, supplementing the traditional scoring methods.\\n\\nEvaluation¶\\n\\nEvaluating with the internlm Model¶\\nFor example, to evaluate using the internlm model, the following command can be used:\\npython run.py configs/eval_needleinahaystack.py --slurm -p partition_name -q auto --max-num-workers 32\\n\\n\\nThis command initiates the evaluation process, where the model attempts to find the specified “needle” in the generated dataset. The parameters -p partition_name -q auto and --max-num-workers 32 specify the Slurm queue and the maximum number of worker processes, respectively.\\n\\n\\nLarge-Scale Text Evaluation with LMDeploy¶\\nWhen evaluating especially long texts (e.g., 200k tokens), conventional methods might lead to memory overload. In such cases, quantized models can be used for evaluation. This can be achieved using the LMDeploy tool (LMDeploy).\\nDetailed information about installing and configuring LMDeploy can be found on its GitHub page. Once installed, the TurboMindModel defined in the configs/eval_needleinahaystack_turbomind.py configuration file can be used for evaluation.\\nBelow is an example configuration in the configs/eval_needleinahaystack_turbomind.py file:\\nfrom opencompass.models.turbomind import TurboMindModel\\nfrom mmengine.config import read_base\\n\\nwith read_base():\\n    from .datasets.cdme.cdme200k import cdme_datasets\\n\\ndatasets = [*cdme_datasets]\\n\\ninternlm_meta_template = dict(round=[\\n    dict(role=\\'HUMAN\\', begin=\\':\\', end=\\'\\\\n\\'),\\n    dict(role=\\'BOT\\', begin=\\':\\', end=\\'<eoa>\\\\n\\', generate=True),\\n],\\n                              eos_token_id=103028)\\n\\nmodels = [\\n    dict(\\n        type=TurboMindModel,\\n        abbr=\\'internlm-chat-20b-turbomind\\',\\n        path=\\'./turbomind\\',\\n        max_out_len=100,\\n        max_seq_len=2048,\\n        batch_size=8,\\n        concurrency=8,\\n        meta_template=internlm_meta_template,\\n        run_cfg=dict(num_gpus=1, num_procs=1),\\n    )\\n]\\n\\n\\nIn this configuration, the TurboMindModel combines the functionality of LMDeploy, suitable for handling large-scale text datasets and effectively reducing memory usage.\\n\\nScore Calculation Method¶\\nIn the CDMEEvaluator class, we use two main methods to calculate scores: levenshtein_distance and score. Here are detailed explanations and implementations of these methods.\\n\\nLevenshtein Distance¶\\nLevenshtein distance is a measure of the difference between two strings. It represents the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\\ndef levenshtein_distance(self, s1, s2):\\n    if len(s1) < len(s2):\\n        return self.levenshtein_distance(s2, s1)\\n\\n    if len(s2) == 0:\\n        return len(s1)\\n\\n    previous_row = range(len(s2) + 1)\\n    for i, c1 in enumerate(s1):\\n        current_row = [i + 1]\\n        for j, c2 in enumerate(s2):\\n            insertions = previous_row[j + 1] + 1\\n            deletions = current_row[j] + 1\\n            substitutions = previous_row[j] + (c1 != c2)\\n            current_row.append(min(insertions, deletions, substitutions))\\n        previous_row = current_row\\n\\n    return previous_row[-1]\\n\\n\\nScore Calculation¶\\nThe score calculation method accepts two lists of predictions and references and calculates the edit distance and score for each pair of prediction and reference.\\ndef score(self, predictions, references):\\n    if len(predictions) != len(references):\\n        return {\"error\": \"predictions and references have different lengths\"}\\n\\n    total_score = 0\\n    details = []\\n    for prediction, reference in zip(predictions, references):\\n        prediction = re.sub(r\\'\\\\s+\\', \\'\\', prediction)\\n        reference = re.sub(r\\'\\\\s+\\', \\'\\', reference)\\n        edit_distance = self.levenshtein_distance(prediction, reference)\\n\\n\\n        max_len = max(len(prediction), len(reference))\\n        score = 100 * (1 - edit_distance /max_len) if max_len != 0 else 100\\n\\n        detail = {\\n            \"pred\": prediction,\\n            \"ref\": reference,\\n            \"edit_distance\": edit_distance,\\n            \"score\": score\\n        }\\n        total_score += score\\n        details.append(detail)\\n\\n    average_score = total_score / len(predictions) if predictions else 0\\n    result = {\"average_score\": average_score, \"details\": details}\\n    return result\\n\\n\\nThis scoring method first removes all whitespace characters from both predictions and references and then calculates the Levenshtein distance between them. The score is calculated as 100 minus the percentage loss based on edit distance. Finally, it returns detailed scores for each prediction and the average score overall.\\n\\nVisualization¶\\nThe tools_needleinahaystack.py script can be used to visualize CSV files. This script supports specifying one or more CSV file paths through the --path parameter and can use the --dataset_length parameter to specify the length of the dataset.\\n\\nUsage Examples¶\\nTo visualize a single CSV file:\\npython tools/tools_needleinahaystack.py --path \\'outputs/default/20231216_161457/summary/summary_20231216_161457.csv\\'\\n\\n\\nTo visualize multiple CSV files:\\npython tools/tools_needleinahaystack.py --path \\'path_to_first_csv.csv\\' \\'path_to_second_csv.csv\\'\\n\\n\\nTo specify the dataset length for visualization, which is used for generating titles in the visualization charts:\\npython tools/tools_needleinahaystack.py --path \\'path_to_csv.csv\\' --dataset_length 200K\\n\\n\\nCurrently, this approach only supports the CDME dataset, and we welcome community contributions for more datasets.\\nIf you use this method, please cite as follows:\\n@misc{2023opencompass,\\n    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},\\n    author={OpenCompass Contributors},\\n    howpublished={\\\\url{https://github.com/open-compass/opencompass}},\\n    year={2023}\\n}\\n\\n@misc{LLMTest_NeedleInAHaystack,\\n  title={LLMTest Needle In A Haystack - Pressure Testing LLMs},\\n  author={gkamradt},\\n  year={2023},\\n  howpublished={\\\\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}}\\n}\\n\\n@misc{wei2023skywork,\\n      title={Skywork: A More Open Bilingual Foundation Model},\\n      author={Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei Lü and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and Xuejie Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},\\n      year={2023},\\n      eprint={2310.19341},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\\n\\nNext \\n Previous\\n\\n\\n      © Copyright 2023, OpenCompass.\\n      \\n      \\n        Revision 120bf8b3.\\n      \\n\\n    Built with Sphinx using a theme provided by Read the\\n      Docs.\\n  \\n\\nNeedle In A Haystack Experiment Evaluation\\nIntroduction to the Needle In A Haystack Test\\nDataset Introduction\\nFile Description\\nEvaluation Steps\\nEnvironment Setup\\nConfiguring the Dataset\\nDataset Configuration Example\\n\\n\\nMulti-Needle Needle In A Haystack Test\\nMulti-Needle Dataset Configuration Example\\nChange in Scoring Mechanism\\n\\n\\nEvaluation\\nEvaluating with the internlm Model\\nLarge-Scale Text Evaluation with LMDeploy\\n\\n\\nScore Calculation Method\\nLevenshtein Distance\\nScore Calculation\\n\\n\\nVisualization\\nUsage Examples\\n\\n\\n Read the Docs\\n      v: latest\\n      \\n\\nVersions\\nlatest\\nstable\\n\\n\\nDownloads\\nepub\\n\\n\\nOn Read the Docs\\n\\nProject Home\\n\\n\\nBuilds\\n\\n      Free document hosting provided by Read the Docs.\\n\\n    \\n\\n@沪ICP备2021009351号-23\\nOpenCompass Open Platform Service Agreement\\n\\nGitHub', metadata={'source': 'https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html', 'title': 'Needle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation', 'language': 'en'}),\n",
       " Document(page_content=\"The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI\\n\\n \\n\\nFree LLM Observability Courses |\\xa0Enroll now!\\n\\n\\nPlatform\\n\\nOverview\\nLeft ColumnMonitorsDashboardsEval & Performance TracingExplainability & Fairness\\nRight ColumnEmbeddings & RAG AnalyzerLLM TracingFine TunePhoenix OSS\\nArize Product Demo: See the Platform in ActionWatch now\\n\\n\\nSolutions\\n\\nOverview\\nLeft ColumnUse CasesComputer VisionRecommender SystemsRegression & ClassificationForecasting\\nRight ColumnIndustriesFinancial ServiceseCommerceMedia & EntertainmentAutonomous VehiclesManufacturingBiotechnology & Pharmaceutical Research\\nCustomersLearn more\\n\\n\\nPricing\\n\\n\\nLearn\\n\\nLeft ColumnResourcesBlogPodcastEventsVideosPaper Readings\\nRight ColumnArize UniversityCertificationBasic and advanced course certificationLLMOpsSelf-guided LLMops learningML ObservabilitySelf-guided observability learning\\nArize CommunityGreat discussions, support, and random acts of swag await!\\nJoin now\\n\\n\\nDocs\\n\\n\\nCompany\\n\\nAbout\\nPartners\\nCareers\\nPress\\nSecurity\\nArize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nSign Up\\n\\n×\\nARIZE GENERATIVE AI COURSE\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tIntro To LLMOps\\t\\t\\n\\nIntro To LLMOps and LLM Observability\\n\\n\\nWhat is LLM Observability?\\n\\nIntroduction To LLMOps\\n\\nFoundation Models\\n\\nOpenAI API\\n\\nLLM Deployment\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Evaluation: Build and Benchmark Evals\\n\\nThe Needle In a Haystack Test\\n\\nEvals from OpenAI\\n\\n\\nTraces and Spans\\n\\n\\nTraces and Spans in Orchestration Frameworks\\n\\n\\nSearch and Retrieval: Build Your Own Chatbot\\n\\n\\nRetrieval Augmented Generation (RAG)\\n\\nBenchmarking Evaluation of LLM RAG\\n\\nRAG Evaluation\\n\\n\\nPrompt Engineering\\n\\n\\nEvaluating Prompts: A Developer’s Guide\\n\\n\\nChains and Agents\\n\\n\\nAI Agents: When and How To Implement\\n\\nLLM Agent: Set Up\\n\\nReAct Architecture | Simple AI Agents\\n\\n\\nLLM Guardrails and Controls for Deployment\\n\\n\\nGuardrails\\n\\n\\nUse Cases\\n\\n\\nStructured Data Extraction\\n\\n Certifications\\n\\nLLM Certification\\nAgents, Tools, and Chains\\nLLM Evaluations\\nTraces and Spans\\nML Observability Fundamentals\\nAdvanced Model Performance Metrics\\n\\n\\nNew to Arize?\\n\\nArize onboarding\\n\\n\\n \\n\\n\\r\\n\\t\\t\\t\\t\\tThe Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems\\t\\t\\t\\t\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tAparna Dhinakaran,\\xa0\\r\\n\\t\\t\\t\\t\\t\\n\\r\\n\\t\\t\\t\\t\\t\\tCo-founder & Chief Product Officer\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\xa0|\\xa0Published February 02, 2024 \\n\\n\\nCo-authored by Evan Jolley\\nIntroduction\\nRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. With RAG’s importance likely to grow, ensuring its effectiveness is paramount.\\nThe evaluation of RAG, therefore, has become a critical part in the development and deployment of these systems. One innovative approach to this challenge is the “Needle in a Haystack” test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here.\\nWhat Is the Needle In a Haystack Test for LLMs?\\nThe “Needle In a Haystack” test is designed to evaluate the performance of LLM RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.\\nOften in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?\\nWe ran this test multiple times across several market leading language models. Let’s take a closer look at the process and overall results, first documented in this X thread.\\nWhat Are the Main Takeaways from The Needle In a Haystack Research?\\n\\nNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.\\nMinute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.\\nWhen building on top of LLMs – especially when those models are connected to private data – it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance, and in turn, customer satisfaction.\\n\\nThe Creation of The Needle In a Haystack Test\\nThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:\\nFigure 1: About 120 tokens and 50% depth\\nThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:\\nFigure 2: ChatGPT-4’s performance\\nAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the ‘needle’ is positioned towards the beginning of the context, the model tends to overlook or “forget” it, whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid. This is truly fascinating stuff.\\nFigure 3: Claude 2.1’s performance\\nAs for Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.\\nAnthropic’s Response\\nIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.\\nFirst, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, amongst an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.\\nSecond, a small edit was made to the prompt template used to query the model.\\nFigure 4: Anthropic’s Prompt Template Update\\nAs you can see, a single line was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.\\nThese changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Our team found this initial research fascinating and decided to run our own set of experiments using the Needle in a Haystack test.\\nOur Research\\nIn conducting our own series of tests, we implemented several modifications to the original experiments.The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our own evaluation library. In doing so we were able to:\\n\\nreduce the testing time from three days to just two hours, and\\nuse rails to search directly for the random number in the output, cutting through any possible wordiness that would decrease a retrieval score.\\n\\nFinally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.\\nThe updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral’s 8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, our team used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:\\nFigure 5: ChatGPT and Mixtral templating\\nAnd for Claude, we tested both previously discussed templates.\\nFigure 6: Claude templating used by Greg Kamradt \\nFigure 7: Revised Claude templating from Anthropic\\nAll code run to complete these tests can be found in this GitHub repository.\\nResults\\nFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2)\\nFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2\\nOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.\\nFigure 9: Comparison of Claude 2.1 results with and without prompting guidance\\nAs for Claude 2.1 with prompting guidance, although we were not able to replicate Anthropic’s results of 98% retrieval accuracy, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.\\n\\nAnd last but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evals.\\nConclusion\\nAs these LLMs become integral to an increasing number of products and services, our ability to evaluate and understand their retrieval capabilities will take on elevated importance.\\nThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral 8x7b MOE greatly outperformed our expectations, and we are excited to see Mistral models continually overperform expectations across our research.\\nFurther articles detailing LLM evaluation methods to follow.\\n\\n\\nOn this page\\n\\n\\n\\r\\n\\t\\t\\tIntroduction\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tWhat is the Needle In a Haystack Test?\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tTakeaways\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tInitial Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tAnthropic's Response\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tOur Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tResults\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tConclusion\\t\\t\\n\\n\\nTry Arize \\n\\n\\nAsk any ML question \\n\\nSign up for our monthly newsletter, The Drift.\\n\\nSubscribe\\n\\nPlatform\\n\\n\\nSolutions\\n\\n\\nDocs\\n\\n\\nPricing\\n\\nLearn\\n\\nCourse\\n\\n\\nCommunity\\n\\n\\nBlog\\n\\nTopics\\n\\nLLM Leaderboard\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Prompt Engineering\\n\\n\\nRAG LLM\\n\\n\\nAI Orchestration\\n\\n\\nMachine Learning Observability\\n\\n\\nML Monitoring\\n\\n\\nModel Monitoring\\n\\n\\nModel Drift\\n\\n\\nKL Divergence\\n\\n\\nJensen Shannon Divergence\\n\\n\\nKolmogorov Smirnov Test\\n\\n\\nAbout\\n\\n\\nCustomers\\n\\n\\nCareers\\n\\n\\nPress\\n\\n\\nSecurity\\n\\n\\nTry now\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nContact\\n\\n\\nPrivacy Policy\\n\\n\\nTwitter\\n\\n\\nLinkedin\\n\\nCopyright © 2024 Arize AI, Inc\\n\\n\\n\\r\\n\\t\\t\\t\\tJoin the Arize ML Observability Community\\t\\t\\t\\t\\t\\n\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to the Arize blog\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to get the latest news, expertise, and product updates from Arize. Your inbox is sacred, so we’ll only curate and send the best stuff.\\t\\t\\t\\t\\t\\n\\n\\n*We’re committed to your privacy. Arize uses the information you provide to contact you about relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our privacy policy.\\n\\n\\n\\r\\n\\t\\t\\t\\tContact us\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tLike what you see? Let’s chat. Fill out this form and we will be in contact with you soon!\", metadata={'source': 'https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/', 'title': 'The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI', 'description': 'Everything you need to know about the popular technique and the importance of evaluating retrieval and model performance throughout development and deployment', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "doc_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Convert documents to Embeddings and store them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Reproducible Long Context (8192) Text Embedde with [ Nomic Embed: A Truly Open Embedding Model](https://blog.nomic.ai/posts/nomic-embed-text-v1)  [🤗](https://huggingface.co/nomic-ai/nomic-embed-text-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-demo-chroma\",\n",
    "    embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chromadb is not presistent by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['0512a70a-d7cc-11ee-b345-8c8d28ad5f1a',\n",
       "  '0512a70b-d7cc-11ee-bc07-8c8d28ad5f1a',\n",
       "  '0512a70c-d7cc-11ee-b9c0-8c8d28ad5f1a',\n",
       "  '0512a70d-d7cc-11ee-87f0-8c8d28ad5f1a'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'description': 'Doing simple retrieval from LLM models at various context lengths to measure accuracy - gkamradt/LLMTest_NeedleInAHaystack',\n",
       "   'language': 'en',\n",
       "   'source': 'https://github.com/gkamradt/LLMTest_NeedleInAHaystack',\n",
       "   'title': 'GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy'},\n",
       "  {'description': 'Retrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. RAG…',\n",
       "   'language': 'en',\n",
       "   'source': 'https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38',\n",
       "   'title': 'The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data Science'},\n",
       "  {'language': 'en',\n",
       "   'source': 'https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html',\n",
       "   'title': 'Needle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation'},\n",
       "  {'description': 'Everything you need to know about the popular technique and the importance of evaluating retrieval and model performance throughout development and deployment',\n",
       "   'language': 'en-US',\n",
       "   'source': 'https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/',\n",
       "   'title': 'The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI'}],\n",
       " 'documents': [\"GitHub - gkamradt/LLMTest_NeedleInAHaystack: Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n\\nSkip to content\\n\\nToggle navigation\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n        Product\\n        \\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\nPackages\\n        Host and manage packages\\n      \\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\nCopilot\\n        Write better code with AI\\n      \\n\\nCode review\\n        Manage code changes\\n      \\n\\nIssues\\n        Plan and track work\\n      \\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\nExplore\\n\\n      All features\\n\\n    \\n\\n      Documentation\\n\\n    \\n\\n      GitHub Skills\\n\\n    \\n\\n      Blog\\n\\n    \\n\\n        Solutions\\n        \\n\\nFor\\n\\n      Enterprise\\n\\n    \\n\\n      Teams\\n\\n    \\n\\n      Startups\\n\\n    \\n\\n      Education\\n\\n    \\n\\n\\nBy Solution\\n\\n      CI/CD & Automation\\n\\n    \\n\\n      DevOps\\n\\n    \\n\\n      DevSecOps\\n\\n    \\n\\n\\nResources\\n\\n      Learning Pathways\\n\\n    \\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n      Customer Stories\\n\\n    \\n\\n      Partners\\n\\n    \\n\\n        Open Source\\n        \\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\nRepositories\\n\\n      Topics\\n\\n    \\n\\n      Trending\\n\\n    \\n\\n      Collections\\n\\n    \\n\\n\\nPricing\\n\\n\\nSearch or jump to...\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n        Search\\n      \\n\\nClear\\n \\n\\n \\n\\n\\n              Search syntax tips\\n \\n\\n\\n        Provide feedback\\n      \\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n \\n\\nName\\n\\n\\nQuery\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n              Sign in\\n            \\n\\n\\n              Sign up\\n            \\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n        gkamradt\\n \\n/\\n\\nLLMTest_NeedleInAHaystack\\n\\nPublic\\n\\n \\n\\nNotifications\\n\\n \\n\\nFork\\n    61\\n\\n\\n \\n\\n\\n          Star\\n 656\\n  \\n\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nLicense\\n\\n     View license\\n    \\n\\n\\n656\\n          stars\\n \\n\\n61\\n          forks\\n \\n\\nBranches\\n \\n\\nTags\\n \\n\\nActivity\\n \\n\\n \\n\\n\\n          Star\\n\\n  \\n\\n \\n\\nNotifications\\n\\n\\nCode\\n\\nIssues\\n1\\n\\n\\nPull requests\\n3\\n\\n\\nActions\\n\\nProjects\\n0\\n\\n\\nSecurity\\n\\nInsights\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n          Code\\n\\n\\n          Issues\\n\\n\\n          Pull requests\\n\\n\\n          Actions\\n\\n\\n          Projects\\n\\n\\n          Security\\n\\n\\n          Insights\\n\\n \\n\\ngkamradt/LLMTest_NeedleInAHaystack\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\n\\n   \\xa0mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History15 CommitsPaulGrahamEssaysPaulGrahamEssays\\xa0\\xa0imgimg\\xa0\\xa0original_resultsoriginal_results\\xa0\\xa0vizviz\\xa0\\xa0.gitignore.gitignore\\xa0\\xa0Anthropic_prompt.txtAnthropic_prompt.txt\\xa0\\xa0LICENSE.txtLICENSE.txt\\xa0\\xa0LLMNeedleHaystackTester.pyLLMNeedleHaystackTester.py\\xa0\\xa0README.mdREADME.md\\xa0\\xa0requirements.txtrequirements.txt\\xa0\\xa0View all filesRepository files navigationREADMELicenseNeedle In A Haystack - Pressure Testing LLMs\\nSupported model providers: OpenAI, Anthropic\\nA simple 'needle in a haystack' analysis to test in-context retrieval ability of long context LLMs.\\nGet the behind the scenes on the overview video.\\n\\ngit clone https://github.com/gkamradt/LLMTest_NeedleInAHaystack.git\\n\\nThe Test\\n\\nPlace a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack')\\nAsk the model to retrieve this statement\\nIterate over various document depths (where the needle is placed) and context lengths to measure performance\\n\\nThis is the code that backed this OpenAI and Anthropic analysis.\\nIf ran and save_results = True, then this script will populate a result/ directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.\\nI've put the results from the original tests in /original_results. I've upgraded the script since those test were ran so the data formats may not match your script results.\\nThe key parameters:\\n\\nneedle - The statement or fact which will be placed in your context ('haystack')\\nhaystack_dir - The directory which contains the text files to load as background context. Only text files are supported\\nretrieval_question - The question with which to retrieve your needle in the background context\\nresults_version - You may want to run your test multiple times for the same combination of length/depth, change the version number if so\\ncontext_lengths_min - The starting point of your context lengths list to iterate\\ncontext_lengths_max - The ending point of your context lengths list to iterate\\ncontext_lengths_num_intervals - The number of intervals between your min/max to iterate through\\ndocument_depth_percent_min - The starting point of your document depths. Should be int > 0\\ndocument_depth_percent_max - The ending point of your document depths. Should be int < 100\\ndocument_depth_percent_intervals - The number of iterations to do between your min/max points\\ndocument_depth_percent_interval_type - Determines the distribution of depths to iterate over. 'linear' or 'sigmoid\\nmodel_provider - 'OpenAI' or 'Anthropic'\\nmodel_name - The name of the model you'd like to test. Should match the exact value which needs to be passed to the api. Ex: gpt-4-1106-preview\\nsave_results - Whether or not you'd like to save your results to file. They will be temporarily saved in the object regardless. True/False\\nsave_contexts - Whether or not you'd like to save your contexts to file. Warning these will get very long. True/False\\n\\nOther Parameters:\\n\\ncontext_lengths - A custom set of context lengths. This will override the values set for context_lengths_min, max, and intervals if set\\ndocument_depth_percents - A custom set of document depths lengths. This will override the values set for document_depth_percent_min, max, and intervals if set\\nopenai_api_key - Must be supplied. GPT-4 is used for evaluation. Can either be passed when creating the object or an environment variable\\nanthropic_api_key - Only needed if testing Anthropic models. Can either be passed when creating the object or an environment variable\\nnum_concurrent_requests - Default: 1. Set higher if you'd like to run more requests in parallel. Keep in mind rate limits.\\nfinal_context_length_buffer - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.\\nseconds_to_sleep_between_completions - Default: None, set # of seconds if you'd like to slow down your requests\\nprint_ongoing_status - Default: True, whether or not to print the status of test as they complete\\n\\nResults Visualization\\nLLMNeedleInHaystackVisualization.ipynb holds the code to make the pivot table visualization. The pivot table was then transferred to Google Slides for custom annotations and formatting. See the google slides version. See an overview of how this viz was created here.\\nOpenAI's GPT-4-128K (Run 11/8/2023)\\n\\nAnthropic's Claude 2.1 (Run 11/21/2023)\\n\\nLicense\\nThis project is licensed under the MIT License - see the LICENSE file for details. Use of this software requires attribution to the original author and project, as detailed in the license.\\n   \\n\\n\\nAbout\\n\\n        Doing simple retrieval from LLM models at various context lengths to measure accuracy\\n      \\nResources\\n\\n        Readme\\n \\nLicense\\n\\n     View license\\n    \\n\\n\\nActivity\\n \\nStars\\n\\n656\\n      stars\\n \\nWatchers\\n\\n9\\n      watching\\n \\nForks\\n\\n61\\n      forks\\n \\n\\n\\n          Report repository\\n \\n\\n    Releases\\n\\nNo releases published\\n\\n\\n    Packages\\n      0\\n\\n\\n        No packages published \\n\\n    Contributors\\n      2\\n\\n\\ngkamradt\\n\\n \\n\\n\\neltociear\\nIkko Eltociear Ashimine\\n\\n \\n\\nLanguages\\n\\nJupyter Notebook\\n87.1%\\n\\nPython\\n12.9%\\n\\nFooter\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n      Manage cookies\\n    \\n\\n      Do not share my personal information\\n    \\n\\n\\n    You can’t perform that action at this time.\",\n",
       "  'The Needle In a Haystack Test. Evaluating the performance of RAG… | by Aparna Dhinakaran | Feb, 2024 | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inImage created by author using Dall-E 3The Needle In a Haystack TestEvaluating the performance of RAG systemsAparna Dhinakaran·FollowPublished inTowards Data Science·9 min read·Feb 15, 2024--ListenShareMy thanks to Greg Kamradt and Evan Jolley for their contributions to this pieceRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses.RAG evaluation, therefore, has become a critical part in the development and deployment of these systems. One new innovative approach to this challenge is the “Needle in a Haystack’’ test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here. This test is designed to evaluate the performance of RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.Often in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?We ran this test multiple times across several major language models. Let’s take a closer look at the process and overall results.TakeawaysNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.Minute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.When building on top of LLMs — especially when those models are connected to private data — it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance.Understanding the Needle In a Haystack TestThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:Figure 1: About 120 tokens and 50% depth | Image by Greg Kamradt on X, used here with author’s permissionThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:Figure 2: ChatGPT-4’s performance | Image by Greg Kamradt on X, used here with author’s permissionAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the “needle” is positioned towards the beginning of the context, the model tends to overlook or “forget” it — whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid.Figure 3: Claude 2.1’s performance | | Image by Greg Kamradt on X, used here with author’s permissionFor Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.Anthropic’s ResponseIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.First, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, along with an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.Second, a small edit was made to the prompt template used to query the model. A single line — here is the most relevant sentence in the context — was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.PROMPT = \"\"\"HUMAN: <context>{context}</context>What is the most fun thing to do in San Francisco based on the context? Don\\'t give information outside the document or repeat our findingsAssistant: here is the most relevant sentence in the context:\"\"\"These changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Finding this initial research fascinating, we decided to run our own set of experiments using the Needle in a Haystack test.Further ExperimentsIn conducting a new series of tests, we implemented several modifications to the original experiments. The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our open source Phoenix evals library (full disclosure: I lead the team that built Phoenix) to reduce the testing time and use rails to search directly for the random number in the output, cutting through wordiness that would decrease a retrieval score. Finally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.The updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral AI’s Mixtral-8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, we used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:SIMPLE_TEMPLATE = \\'\\'\\'    You are a helpful AI bot that answers questions for a user. Keep your responses short and direct.    The following is a set of context and a question that will relate to the context.   #CONTEXT   {context}   #ENDCONTEXT   #QUESTION   {question} Don’t give information outside the document or repeat your findings. If the information is not available in the context respond UNANSWERABLEFor Claude, we tested both previously discussed templates.ANTHROPIC_TEMPLATE_ORIGINAL = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I’m going to give you the text of some essays. Amidst the essays (“the haystack”) I’ve inserted a sentence (“the needle”) that contains an answer to the user’s question. Here\\'s the question:   <question>{question}</question>   Here’s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you’ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can’t find the answer return the single word UNANSWERABLE   Assistant: \\'\\'\\'ANTHROPIC_TEMPLATE_REV2 = \\'\\'\\' Human: You are a close-reading bot with a great memory who answers questions for users. I\\'m going to give you the text of some essays. Amidst the essays (\"the haystack\") I\\'ve inserted a sentence (\"the needle\") that contains an answer to the user\\'s question. Here\\'s the question:   <question>{question}</question>   Here\\'s the text of the essays. The answer appears in it somewhere.   <haystack>   {context}   </haystack>   Now that you\\'ve read the context, please answer the user\\'s question, repeated one more time for reference:   <question>{question}</question>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside <most_relevant_sentence> XML tags. Then, put your answer in <answer> tags. Base your answer strictly on the context, without reference to outside information. Thank you.    If you can\\'t find the answer return the single word UNANSWERABLE   Assistant: Here is the most relevant sentence in the context:\\'\\'\\'All code run to complete these tests can be found in this GitHub repository.ResultsFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2) | Image by authorFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2 | Image by authorOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.Figure 9: Comparison of Claude 2.1 results with and without prompting guidanceAlthough we were not able to replicate Anthropic’s results of 98% retrieval accuracy for Claude 2.1 with prompting guidance, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.Figure 10: Mixtral results | Image by authorLast but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evaluations.ConclusionThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral MOE greatly outperformed our expectations, and we are excited to see Mixtral models continually overperform expectations.Llm EvaluationRetrieval AugmentedMistralOpenAIHands On Tutorials----FollowWritten by Aparna Dhinakaran2.3K Followers·Writer for Towards Data ScienceCo-Founder and CPO of Arize AI. Formerly Computer Vision PhD at Cornell, Uber Machine Learning, UC Berkeley AI Research.FollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams',\n",
       "  'Needle In A Haystack Experiment Evaluation — OpenCompass 0.2.2 documentation\\n\\nGitHub\\n\\n\\nHomepage\\n\\n\\n                Leaderboard\\n              \\n\\n\\nLLM \\n\\nVLM \\n\\n\\nTable of Contents\\n\\n            latest\\n          \\n\\n\\nGet Started\\n\\nInstallation\\nDataset Preparation\\nQuick Start\\nFAQ\\n\\nUser Guides\\n\\nOverview\\nLearn About Config\\nConfigure Datasets\\nPrepare Models\\nEfficient Evaluation\\nTask Execution and Monitoring\\nMetric Calculation\\nResults Summary\\n\\nPrompt\\n\\nPrompt Overview\\nPrompt Template\\nMeta Template\\nChain of Thought\\n\\nAdvanced Guides\\n\\nAdd a dataset\\nCustom Dataset Tutorial\\nAdd a Model\\nEvaluation with LMDeploy\\nEvaluation with Lightllm\\nCode Evaluation Tutorial\\nCode Evaluation Docker Tutorial\\nMulti-modality Evaluation\\nPrompt Attack\\nLong Context Evaluation Guidance\\nSubjective Evaluation Guidance\\nCircularEval\\nData Contamination Assessment\\nNeedle In A Haystack Experiment Evaluation\\n\\nTools\\n\\nUseful Tools\\n\\nNotes\\n\\nContributing to OpenCompass\\n\\n            Docs\\n         >\\n      \\nNeedle In A Haystack Experiment Evaluation\\n\\n 以中文阅读\\n\\n        Shortcuts\\n      \\n\\nNeedle In A Haystack Experiment Evaluation¶\\n\\nIntroduction to the Needle In A Haystack Test¶\\nThe Needle In A Haystack test, inspired by NeedleInAHaystack, is a method to evaluate the long-text information extraction ability of Large Language Models (LLMs). It involves randomly inserting key information at various points in a long text to form a prompt for LLMs. This test assesses the fundamental ability of LLMs to understand long texts by extracting critical information from them.\\n\\n\\nDataset Introduction¶\\nThe Skywork/ChineseDomainModelingEval dataset includes high-quality Chinese articles published between September and October 2023, covering multiple domains. These articles ensure a fair and challenging benchmark for testing.\\n\\n\\nFile Description¶\\nThe dataset includes files specific to various domains:\\n\\nzh_finance.jsonl - Finance\\nzh_game.jsonl - Gaming\\nzh_government.jsonl - Government\\nzh_movie.jsonl - Movies\\nzh_tech.jsonl - Technology\\nzh_general.jsonl - General\\n\\nThese files are used to assess the LLM’s understanding of different specific domains.\\n\\nEvaluation Steps¶\\n\\nDownload the dataset from Skywork/ChineseDomainModelingEval.\\nPlace the downloaded files in opencompass/data/CDME/. The expected file structure in the CDME directory is as follows:\\nopencompass/\\n├── configs\\n├── docs\\n├── data\\n│   └── CDME\\n│       ├── processed\\n│       ├── README.md\\n│       ├── zh_finance.jsonl\\n│       ├── zh_game.jsonl\\n│       ├── zh_general.jsonl\\n│       ├── zh_government.jsonl\\n│       ├── zh_movie.jsonl\\n│       └── zh_tech.jsonl\\n├── LICENSE\\n├── opencompass\\n├── outputs\\n├── run.py\\n├── more...\\n\\n\\nEnvironment Setup¶\\nconda create --name opencompass python=3.10 pytorch torchvision pytorch-cuda -c nvidia -c pytorch -y\\nconda activate opencompass\\ngit clone https://github.com/open-compass/opencompass opencompass\\ncd opencompass\\npip install -e .\\n\\n\\nConfiguring the Dataset¶\\nIn the latest version, datasets are no longer generated by running scripts but dynamically defined and loaded through configuration files. Users need to specify dataset parameters in the configuration file according to their needs, offering greater flexibility and customization options.\\n\\nDataset Configuration Example¶\\nHere is an example of dataset configuration, showing how to define a dataset in the configs/datasets/cdme/cdme8k.py configuration file. This example demonstrates a Chinese dataset configuration with a length of 8000 tokens:\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            \\'abbr\\': f\\'CDME_Length{original_context_length}Depth{int(depth_percent)}\\',\\n            \\'type\\': CDMEDataset,\\n            \\'path\\': base_path,\\n            \\'length\\': original_context_length,\\n            \\'depth\\': int(depth_percent),\\n            \\'tokenizer_model\\': \\'gpt-4\\',\\n            \\'file_list\\': file_list,\\n            \\'num_repeats_per_file\\': 10,\\n            \\'length_buffer\\': 200,\\n            \\'guide\\': True,\\n            \\'language\\': \\'Chinese\\',\\n            \\'needle\\': \\'\\\\n小明最喜欢的实习的地点就是上海人工智能实验室。\\\\n\\',\\n            \\'retrieval_question\\': \\'小明最喜欢的实习地点是哪里？请按照“小明最喜欢的实习地点就是________。”的格式回答。\\',\\n            \\'reader_cfg\\': cdme_reader_cfg,\\n            \\'infer_cfg\\': cdme_infer_cfg,\\n            \\'eval_cfg\\': cdme_eval_cfg\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, the main parameters include:\\n\\nabbr: Abbreviation of the dataset.\\ntype: Dataset type.\\npath: Path to the dataset files.\\nlength: Context length in tokens.\\ndepth: Depth percentage of the document.\\ntokenizer_model: Tokenizer model used.\\nfile_list: List of data source files.\\nnum_repeats_per_file: Number of repeats per file.\\nlength_buffer: Length buffer.\\nguide: Whether it’s a guided dataset.\\nlanguage: Language of the dataset.\\nneedle: Specific text to find in the dataset (the ‘needle’).\\nretrieval_question: Question used to prompt the model for retrieval.\\nreader_cfg, infer_cfg, eval_cfg: Configurations for reading, inference, and evaluation, respectively.\\n\\nBy defining these parameters in the configuration file, you can flexibly create datasets that suit your needs. Configuration files offer a highly customizable and scalable way to manage the generation and use of datasets.\\n\\nMulti-Needle Needle In A Haystack Test¶\\nThe latest version introduces the multi-needle Needle In A Haystack test, allowing multiple different needles (text snippets) to be inserted into the same dataset. These needles are inserted in sequence according to a given depth parameter. Compared to the single-needle test, the multi-needle test provides a more complex data processing scenario.\\n\\nMulti-Needle Dataset Configuration Example¶\\nHere is an example of configuring a multi-needle dataset, showing how to define a multi-needle dataset in the configs/datasets/cdme/multi_needle/cdme8k_cot3_italy.py configuration file. This example demonstrates a dataset configuration with three needles:\\n# Basic dataset configuration\\nbase_path = \\'./data/CDME\\'\\nfile_list = [\\'zh_finance.jsonl\\']\\n\\n# Definition of Needles\\nneedles = [\\n    \\'\\\\n意大利的佛罗伦萨有一家名为\"La Giostra\"的餐馆，是整个佛罗伦萨中排行第一的餐馆。\\\\n\\',\\n    \\'\"La Giostra\"餐馆的特色菜肴是松露奶酪通心粉。\\',\\n    \\'松露奶酪通心粉是该家餐馆的有着意大利皇室烹饪血统的大厨Jack制作\\'\\n]\\n\\n\\n# Configuration parameters\\nretrieval_question = (\"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫什么？\"\\n                      \"请按照\\'制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫______。\\'的格式回答。\")\\nanswer = \"制作佛罗伦萨中排行第一的餐馆的特色菜肴的人叫Jack\"\\nkeyword = \"Jack\"\\ndiff = 25\\n\\n# Dataset generation loop\\nfor original_context_length in context_lengths:\\n    for depth_percent in generate_depth_percents(\\n            document_depth_percent_intervals,\\n            document_depth_percent_interval_type):\\n        dataset_dict = {\\n            # Other configuration items...\\n            \\'needles\\': needles,\\n            \\'diff\\': diff,\\n            \\'keyword\\': keyword,\\n            # Other configuration items...\\n        }\\n        cdme_datasets.append(dataset_dict)\\n\\n\\nIn this configuration, in addition to the standard parameters, the main new parameters include:\\n\\nneedles: A list containing multiple strings, each representing a needle to be inserted.\\ndiff: Defines the depth increment for subsequent needles relative to the first needle.\\nkeyword: A keyword used for score correction during the evaluation process.\\n\\nChange in Scoring Mechanism¶\\nIn the source code of opencompass/datasets/cdme/cdme_multi.py, the scoring mechanism for multi-needle datasets differs. The following code segment has been added to adjust the scores based on the keyword in the predictions:\\nif keyword in prediction:\\n    print(f\\'{keyword} is in {prediction}\\')\\n    score = 100\\nelse:\\n    print(f\\'{keyword} is not in {prediction}\\')\\n    score = 0.2 * score\\n\\n\\nThis code means that if the keyword is present in the prediction, it will be awarded a high score (e.g., 100). If not, the score will be significantly reduced (20% of the original score). This scoring mechanism places more emphasis on the accuracy of keywords, supplementing the traditional scoring methods.\\n\\nEvaluation¶\\n\\nEvaluating with the internlm Model¶\\nFor example, to evaluate using the internlm model, the following command can be used:\\npython run.py configs/eval_needleinahaystack.py --slurm -p partition_name -q auto --max-num-workers 32\\n\\n\\nThis command initiates the evaluation process, where the model attempts to find the specified “needle” in the generated dataset. The parameters -p partition_name -q auto and --max-num-workers 32 specify the Slurm queue and the maximum number of worker processes, respectively.\\n\\n\\nLarge-Scale Text Evaluation with LMDeploy¶\\nWhen evaluating especially long texts (e.g., 200k tokens), conventional methods might lead to memory overload. In such cases, quantized models can be used for evaluation. This can be achieved using the LMDeploy tool (LMDeploy).\\nDetailed information about installing and configuring LMDeploy can be found on its GitHub page. Once installed, the TurboMindModel defined in the configs/eval_needleinahaystack_turbomind.py configuration file can be used for evaluation.\\nBelow is an example configuration in the configs/eval_needleinahaystack_turbomind.py file:\\nfrom opencompass.models.turbomind import TurboMindModel\\nfrom mmengine.config import read_base\\n\\nwith read_base():\\n    from .datasets.cdme.cdme200k import cdme_datasets\\n\\ndatasets = [*cdme_datasets]\\n\\ninternlm_meta_template = dict(round=[\\n    dict(role=\\'HUMAN\\', begin=\\':\\', end=\\'\\\\n\\'),\\n    dict(role=\\'BOT\\', begin=\\':\\', end=\\'<eoa>\\\\n\\', generate=True),\\n],\\n                              eos_token_id=103028)\\n\\nmodels = [\\n    dict(\\n        type=TurboMindModel,\\n        abbr=\\'internlm-chat-20b-turbomind\\',\\n        path=\\'./turbomind\\',\\n        max_out_len=100,\\n        max_seq_len=2048,\\n        batch_size=8,\\n        concurrency=8,\\n        meta_template=internlm_meta_template,\\n        run_cfg=dict(num_gpus=1, num_procs=1),\\n    )\\n]\\n\\n\\nIn this configuration, the TurboMindModel combines the functionality of LMDeploy, suitable for handling large-scale text datasets and effectively reducing memory usage.\\n\\nScore Calculation Method¶\\nIn the CDMEEvaluator class, we use two main methods to calculate scores: levenshtein_distance and score. Here are detailed explanations and implementations of these methods.\\n\\nLevenshtein Distance¶\\nLevenshtein distance is a measure of the difference between two strings. It represents the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\\ndef levenshtein_distance(self, s1, s2):\\n    if len(s1) < len(s2):\\n        return self.levenshtein_distance(s2, s1)\\n\\n    if len(s2) == 0:\\n        return len(s1)\\n\\n    previous_row = range(len(s2) + 1)\\n    for i, c1 in enumerate(s1):\\n        current_row = [i + 1]\\n        for j, c2 in enumerate(s2):\\n            insertions = previous_row[j + 1] + 1\\n            deletions = current_row[j] + 1\\n            substitutions = previous_row[j] + (c1 != c2)\\n            current_row.append(min(insertions, deletions, substitutions))\\n        previous_row = current_row\\n\\n    return previous_row[-1]\\n\\n\\nScore Calculation¶\\nThe score calculation method accepts two lists of predictions and references and calculates the edit distance and score for each pair of prediction and reference.\\ndef score(self, predictions, references):\\n    if len(predictions) != len(references):\\n        return {\"error\": \"predictions and references have different lengths\"}\\n\\n    total_score = 0\\n    details = []\\n    for prediction, reference in zip(predictions, references):\\n        prediction = re.sub(r\\'\\\\s+\\', \\'\\', prediction)\\n        reference = re.sub(r\\'\\\\s+\\', \\'\\', reference)\\n        edit_distance = self.levenshtein_distance(prediction, reference)\\n\\n\\n        max_len = max(len(prediction), len(reference))\\n        score = 100 * (1 - edit_distance /max_len) if max_len != 0 else 100\\n\\n        detail = {\\n            \"pred\": prediction,\\n            \"ref\": reference,\\n            \"edit_distance\": edit_distance,\\n            \"score\": score\\n        }\\n        total_score += score\\n        details.append(detail)\\n\\n    average_score = total_score / len(predictions) if predictions else 0\\n    result = {\"average_score\": average_score, \"details\": details}\\n    return result\\n\\n\\nThis scoring method first removes all whitespace characters from both predictions and references and then calculates the Levenshtein distance between them. The score is calculated as 100 minus the percentage loss based on edit distance. Finally, it returns detailed scores for each prediction and the average score overall.\\n\\nVisualization¶\\nThe tools_needleinahaystack.py script can be used to visualize CSV files. This script supports specifying one or more CSV file paths through the --path parameter and can use the --dataset_length parameter to specify the length of the dataset.\\n\\nUsage Examples¶\\nTo visualize a single CSV file:\\npython tools/tools_needleinahaystack.py --path \\'outputs/default/20231216_161457/summary/summary_20231216_161457.csv\\'\\n\\n\\nTo visualize multiple CSV files:\\npython tools/tools_needleinahaystack.py --path \\'path_to_first_csv.csv\\' \\'path_to_second_csv.csv\\'\\n\\n\\nTo specify the dataset length for visualization, which is used for generating titles in the visualization charts:\\npython tools/tools_needleinahaystack.py --path \\'path_to_csv.csv\\' --dataset_length 200K\\n\\n\\nCurrently, this approach only supports the CDME dataset, and we welcome community contributions for more datasets.\\nIf you use this method, please cite as follows:\\n@misc{2023opencompass,\\n    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},\\n    author={OpenCompass Contributors},\\n    howpublished={\\\\url{https://github.com/open-compass/opencompass}},\\n    year={2023}\\n}\\n\\n@misc{LLMTest_NeedleInAHaystack,\\n  title={LLMTest Needle In A Haystack - Pressure Testing LLMs},\\n  author={gkamradt},\\n  year={2023},\\n  howpublished={\\\\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}}\\n}\\n\\n@misc{wei2023skywork,\\n      title={Skywork: A More Open Bilingual Foundation Model},\\n      author={Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei Lü and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and Xuejie Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},\\n      year={2023},\\n      eprint={2310.19341},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\\n\\nNext \\n Previous\\n\\n\\n      © Copyright 2023, OpenCompass.\\n      \\n      \\n        Revision 120bf8b3.\\n      \\n\\n    Built with Sphinx using a theme provided by Read the\\n      Docs.\\n  \\n\\nNeedle In A Haystack Experiment Evaluation\\nIntroduction to the Needle In A Haystack Test\\nDataset Introduction\\nFile Description\\nEvaluation Steps\\nEnvironment Setup\\nConfiguring the Dataset\\nDataset Configuration Example\\n\\n\\nMulti-Needle Needle In A Haystack Test\\nMulti-Needle Dataset Configuration Example\\nChange in Scoring Mechanism\\n\\n\\nEvaluation\\nEvaluating with the internlm Model\\nLarge-Scale Text Evaluation with LMDeploy\\n\\n\\nScore Calculation Method\\nLevenshtein Distance\\nScore Calculation\\n\\n\\nVisualization\\nUsage Examples\\n\\n\\n Read the Docs\\n      v: latest\\n      \\n\\nVersions\\nlatest\\nstable\\n\\n\\nDownloads\\nepub\\n\\n\\nOn Read the Docs\\n\\nProject Home\\n\\n\\nBuilds\\n\\n      Free document hosting provided by Read the Docs.\\n\\n    \\n\\n@沪ICP备2021009351号-23\\nOpenCompass Open Platform Service Agreement\\n\\nGitHub',\n",
       "  \"The Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems - Arize AI\\n\\n \\n\\nFree LLM Observability Courses |\\xa0Enroll now!\\n\\n\\nPlatform\\n\\nOverview\\nLeft ColumnMonitorsDashboardsEval & Performance TracingExplainability & Fairness\\nRight ColumnEmbeddings & RAG AnalyzerLLM TracingFine TunePhoenix OSS\\nArize Product Demo: See the Platform in ActionWatch now\\n\\n\\nSolutions\\n\\nOverview\\nLeft ColumnUse CasesComputer VisionRecommender SystemsRegression & ClassificationForecasting\\nRight ColumnIndustriesFinancial ServiceseCommerceMedia & EntertainmentAutonomous VehiclesManufacturingBiotechnology & Pharmaceutical Research\\nCustomersLearn more\\n\\n\\nPricing\\n\\n\\nLearn\\n\\nLeft ColumnResourcesBlogPodcastEventsVideosPaper Readings\\nRight ColumnArize UniversityCertificationBasic and advanced course certificationLLMOpsSelf-guided LLMops learningML ObservabilitySelf-guided observability learning\\nArize CommunityGreat discussions, support, and random acts of swag await!\\nJoin now\\n\\n\\nDocs\\n\\n\\nCompany\\n\\nAbout\\nPartners\\nCareers\\nPress\\nSecurity\\nArize AI Introduces Next Generation of Its ML Observability Platform, Goes Self-Serve Learn more\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nSign Up\\n\\n×\\nARIZE GENERATIVE AI COURSE\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tIntro To LLMOps\\t\\t\\n\\nIntro To LLMOps and LLM Observability\\n\\n\\nWhat is LLM Observability?\\n\\nIntroduction To LLMOps\\n\\nFoundation Models\\n\\nOpenAI API\\n\\nLLM Deployment\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Evaluation: Build and Benchmark Evals\\n\\nThe Needle In a Haystack Test\\n\\nEvals from OpenAI\\n\\n\\nTraces and Spans\\n\\n\\nTraces and Spans in Orchestration Frameworks\\n\\n\\nSearch and Retrieval: Build Your Own Chatbot\\n\\n\\nRetrieval Augmented Generation (RAG)\\n\\nBenchmarking Evaluation of LLM RAG\\n\\nRAG Evaluation\\n\\n\\nPrompt Engineering\\n\\n\\nEvaluating Prompts: A Developer’s Guide\\n\\n\\nChains and Agents\\n\\n\\nAI Agents: When and How To Implement\\n\\nLLM Agent: Set Up\\n\\nReAct Architecture | Simple AI Agents\\n\\n\\nLLM Guardrails and Controls for Deployment\\n\\n\\nGuardrails\\n\\n\\nUse Cases\\n\\n\\nStructured Data Extraction\\n\\n Certifications\\n\\nLLM Certification\\nAgents, Tools, and Chains\\nLLM Evaluations\\nTraces and Spans\\nML Observability Fundamentals\\nAdvanced Model Performance Metrics\\n\\n\\nNew to Arize?\\n\\nArize onboarding\\n\\n\\n \\n\\n\\r\\n\\t\\t\\t\\t\\tThe Needle In a Haystack Test: Evaluating the Performance of LLM RAG Systems\\t\\t\\t\\t\\n\\n\\n\\r\\n\\t\\t\\t\\t\\t\\tAparna Dhinakaran,\\xa0\\r\\n\\t\\t\\t\\t\\t\\n\\r\\n\\t\\t\\t\\t\\t\\tCo-founder & Chief Product Officer\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\xa0|\\xa0Published February 02, 2024 \\n\\n\\nCo-authored by Evan Jolley\\nIntroduction\\nRetrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses. With RAG’s importance likely to grow, ensuring its effectiveness is paramount.\\nThe evaluation of RAG, therefore, has become a critical part in the development and deployment of these systems. One innovative approach to this challenge is the “Needle in a Haystack” test, first outlined by Greg Kamradt in this X post and discussed in detail on his YouTube here.\\nWhat Is the Needle In a Haystack Test for LLMs?\\nThe “Needle In a Haystack” test is designed to evaluate the performance of LLM RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.\\nOften in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?\\nWe ran this test multiple times across several market leading language models. Let’s take a closer look at the process and overall results, first documented in this X thread.\\nWhat Are the Main Takeaways from The Needle In a Haystack Research?\\n\\nNot all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.\\nMinute differences in prompts can lead to drastically different outcomes across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.\\nWhen building on top of LLMs – especially when those models are connected to private data – it is necessary to evaluate retrieval and model performance throughout development and deployment. Seemingly insignificant differences can lead to incredibly large differences in performance, and in turn, customer satisfaction.\\n\\nThe Creation of The Needle In a Haystack Test\\nThe Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from essays by Paul Graham, similar to this:\\nFigure 1: About 120 tokens and 50% depth\\nThe models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:\\nFigure 2: ChatGPT-4’s performance\\nAs you can see, ChatGPT’s performance begins to decline at <64k tokens and sharply falls at 100k and over. Interestingly, if the ‘needle’ is positioned towards the beginning of the context, the model tends to overlook or “forget” it, whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid. This is truly fascinating stuff.\\nFigure 3: Claude 2.1’s performance\\nAs for Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.\\nAnthropic’s Response\\nIn response to these findings, Anthropic published an article detailing their re-run of this test with a few key changes.\\nFirst, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, amongst an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.\\nSecond, a small edit was made to the prompt template used to query the model.\\nFigure 4: Anthropic’s Prompt Template Update\\nAs you can see, a single line was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.\\nThese changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Our team found this initial research fascinating and decided to run our own set of experiments using the Needle in a Haystack test.\\nOur Research\\nIn conducting our own series of tests, we implemented several modifications to the original experiments.The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our own evaluation library. In doing so we were able to:\\n\\nreduce the testing time from three days to just two hours, and\\nuse rails to search directly for the random number in the output, cutting through any possible wordiness that would decrease a retrieval score.\\n\\nFinally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.\\nThe updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral’s 8X7B-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, our team used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:\\nFigure 5: ChatGPT and Mixtral templating\\nAnd for Claude, we tested both previously discussed templates.\\nFigure 6: Claude templating used by Greg Kamradt \\nFigure 7: Revised Claude templating from Anthropic\\nAll code run to complete these tests can be found in this GitHub repository.\\nResults\\nFigure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2)\\nFigure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2\\nOur results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.\\nFigure 9: Comparison of Claude 2.1 results with and without prompting guidance\\nAs for Claude 2.1 with prompting guidance, although we were not able to replicate Anthropic’s results of 98% retrieval accuracy, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.\\n\\nAnd last but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evals.\\nConclusion\\nAs these LLMs become integral to an increasing number of products and services, our ability to evaluate and understand their retrieval capabilities will take on elevated importance.\\nThe Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. Our research concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral 8x7b MOE greatly outperformed our expectations, and we are excited to see Mistral models continually overperform expectations across our research.\\nFurther articles detailing LLM evaluation methods to follow.\\n\\n\\nOn this page\\n\\n\\n\\r\\n\\t\\t\\tIntroduction\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tWhat is the Needle In a Haystack Test?\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tTakeaways\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tInitial Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tAnthropic's Response\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tOur Research\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tResults\\t\\t\\n\\n\\n\\r\\n\\t\\t\\tConclusion\\t\\t\\n\\n\\nTry Arize \\n\\n\\nAsk any ML question \\n\\nSign up for our monthly newsletter, The Drift.\\n\\nSubscribe\\n\\nPlatform\\n\\n\\nSolutions\\n\\n\\nDocs\\n\\n\\nPricing\\n\\nLearn\\n\\nCourse\\n\\n\\nCommunity\\n\\n\\nBlog\\n\\nTopics\\n\\nLLM Leaderboard\\n\\n\\nLLM Evaluation\\n\\n\\nLLM Prompt Engineering\\n\\n\\nRAG LLM\\n\\n\\nAI Orchestration\\n\\n\\nMachine Learning Observability\\n\\n\\nML Monitoring\\n\\n\\nModel Monitoring\\n\\n\\nModel Drift\\n\\n\\nKL Divergence\\n\\n\\nJensen Shannon Divergence\\n\\n\\nKolmogorov Smirnov Test\\n\\n\\nAbout\\n\\n\\nCustomers\\n\\n\\nCareers\\n\\n\\nPress\\n\\n\\nSecurity\\n\\n\\nTry now\\n\\n\\nBook a Demo\\n\\n\\nSign In\\n\\n\\nContact\\n\\n\\nPrivacy Policy\\n\\n\\nTwitter\\n\\n\\nLinkedin\\n\\nCopyright © 2024 Arize AI, Inc\\n\\n\\n\\r\\n\\t\\t\\t\\tJoin the Arize ML Observability Community\\t\\t\\t\\t\\t\\n\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to the Arize blog\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tSubscribe to get the latest news, expertise, and product updates from Arize. Your inbox is sacred, so we’ll only curate and send the best stuff.\\t\\t\\t\\t\\t\\n\\n\\n*We’re committed to your privacy. Arize uses the information you provide to contact you about relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our privacy policy.\\n\\n\\n\\r\\n\\t\\t\\t\\tContact us\\t\\t\\t\\t\\t\\n\\n\\r\\n\\t\\t\\t\\tLike what you see? Let’s chat. Fill out this form and we will be in contact with you soon!\"],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000028553D7FF10>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Before RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Needle in a haystack\" is an idiom that means something that is very difficult to find or locate, often because it is hidden among a lot of other things. The phrase is often used to describe a situation where someone is searching for something specific, but it is hard to find due to the large amount of irrelevant or distracting information present.\n",
      "\n",
      "The phrase is thought to have originated from the idea of trying to find a small needle buried in a large pile of hay. It is a metaphor for a difficult task, as finding a single needle in a huge amount of hay would be almost impossible without some sort of guidance or hint.\n",
      "\n",
      "Examples:\n",
      "\n",
      "* \"I've been searching for my car keys all morning, but they're like a needle in a haystack.\"\n",
      "* \"Finding the perfect job is like looking for a needle in a haystack – it's out there, but you have to sift through a lot of other options first.\"\n",
      "* \"The needle in the haystack was the last thing I expected to find in this box of random items.\"\n",
      "\n",
      "In summary, \"needle in a haystack\" is an idiom that refers to something that is very difficult to find or locate due to its small size or insignificance among other things.\n"
     ]
    }
   ],
   "source": [
    "before_rag_template = \"What is {topic}\"\n",
    "before_rag_prompt = ChatPromptTemplate.from_template(before_rag_template)\n",
    "before_rag_chain = before_rag_prompt | llm | StrOutputParser()\n",
    "print(before_rag_chain.invoke({\"topic\": \"needle in haystack\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "'|' operator: Instead of its traditional use for bitwise operations, here it's used to pass data from one object to the next in the pipeline. The | operator has been overloaded by the classes of the objects involved "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. After RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"Needle in a Haystack\" test is a method to assess an AI model's ability to extract relevant information from a large amount of text or data, much like finding a specific needle in a haystack. In this context, the \"haystack\" represents the vast amount of text or data, and the \"needle\" represents the specific piece of information that the model needs to retrieve.\n",
      "\n",
      "The test involves providing the AI model with a large amount of context, such as a document or a group of documents, and then asking it to find a specific piece of information within that context. The model's ability to locate the needle in the haystack is measured by its precision and recall in retrieving the correct information.\n",
      "\n",
      "The test is useful for evaluating the performance of large language models (LLMs) and other natural language processing (NLP) models, as it can help identify their strengths and weaknesses in parsing context to find needed information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "after_rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
    "after_rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | after_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(after_rag_chain.invoke(\"whats needle in haystack?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. gradio (UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[gradio](https://www.gradio.app/) fastest (& simplest) way to demo ur ML model with friendly web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(fn=lambda x, y: x + y, \n",
    "             inputs=[\"number\", \"number\"], \n",
    "             outputs=\"number\").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def process_input(urls, question):\n",
    "    llm = ChatOllama(model=\"llama2\", temperature=0.8)  # Initialize ChatOllama model\n",
    "    \n",
    "    # Convert string of URLs to list by splitting on new lines\n",
    "    urls_list = urls.split(\"\\n\")\n",
    "\n",
    "    # Load documents from URLs using WebBaseLoader and compile them into a list\n",
    "    docs = [WebBaseLoader(url).load() for url in urls_list]\n",
    "\n",
    "    # Flatten the list of lists into a single list\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    \n",
    "    # Initialize a text splitter with specified chunk size and overlap\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=8000, chunk_overlap=100)\n",
    "    # Split the loaded documents into chunks\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Create a Chroma vector store from the document splits, using Ollama embeddings\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=\"rag-ollama\",\n",
    "        embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),\n",
    "    )\n",
    "    # Convert the Chroma vector store into a retriever\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Template for generating responses, emphasizing brevity\n",
    "    rag_template = \"\"\"Laconicy answer the question based only on the following context:\n",
    "    {context}.\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a chat prompt template from the template string\n",
    "    rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "    # Chain the components to create a processing pipeline\n",
    "    after_rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}  # Pass context and question to the pipeline\n",
    "        | rag_prompt  # Apply the chat prompt template\n",
    "        | llm  # Pass the result to the LLM for processing\n",
    "        | StrOutputParser()  # Parse the model's output to a string\n",
    "    )\n",
    "    # Invoke the processing chain with the input question and return the result\n",
    "    return after_rag_chain.invoke(question)\n",
    "\n",
    "# Define a Gradio interface with specified inputs and output\n",
    "iface = gr.Interface(fn=process_input,\n",
    "                     inputs=[gr.Textbox(label=\"Enter URLs separated by new lines\"), \n",
    "                             gr.Textbox(label=\"Question\")],\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Document Query with Ollama\",\n",
    "                     description=\"Enter URLs and a question to query the documents.\")\n",
    "iface.launch()  # Launch the Gradio interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Flag\" button writes input and ansewer into flagged/log.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 7. Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook demo.ipynb to slides\n",
      "[NbConvertApp] Writing 956174 bytes to demo.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert demo.ipynb  --to slides\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
